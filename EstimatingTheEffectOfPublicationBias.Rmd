---
title: "EstimatingPublicationBias"
output:
  word_document
---

```{r message=FALSE, warning=FALSE, include=FALSE}
source(file = 'Data/data_collection_cleaning.R')
source(file = 'simplifiedAnalysisScript.R')
options(scipen = 1, digits = 2)
knitr::opts_chunk$set(fig.width=8, fig.height=5, echo =FALSE) 
options(knitr.kable.NA = '')
```

## Introduction
See manuscript

## Methods 
### Data extraction
All eight published or in press large scale replication projects performed within in the behavioral science research literature were included in the current research (see Table 1 for a list of the included studies). The source of each replicated effect, reported test statistics, effect sizes, sample sizes, and p-values were extracted for each original and replication study. Several of the large scale replication projects did not present the original test statistics and p values (e.g., Many Labs 1 and 3 {Klein, 2014 #988;Ebersole, 2016 #985}). In these cases, these values were manually extracted from the original articles. When sample sizes for original studies were not reported in the data provided by each replication project they were manually extracted from original articles where possible.

For all analyses, the original and replication effect sizes were transformed to Fisher z-transformed correlation coefficients following the methods used in Open Science Collaboration (2015, see Supplementary Materials 5 for details). This conversion used data from the replication project whenever possible (i.e., whenever effect sizes were reported in correlation coefficients in a summary table or in a project’s online data this was directly converted to Fisher z values). If the study-level results were not reported as correlation coefficients, Cohen’s d values, as t-tests, or as F statistics in the original or replication project we excluded the result from this analysis (e.g., cases when no effect size was reported in the original study or in the replication project data set). In cases where sample sizes were not reported per group, sample sizes among groups were assumed to be equal in these conversions. For each of the Many Labs projects the top level result was used (i.e., the results of the analysis that collapsed the data across the multiple labs). See Supplementary Materials 1 for a comprehensive account of exclusions and study specific extraction details for each replication project. An original and replication effect size that could be converted to a Fisher z-score, along with sample sizes for original and replication studies, was extracted for a total of `r sum(!is.na(allData$fis.o) & !is.na(allData$fis.r) & !is.na(allData$n.o) & !is.na(allData$n.r) )` pairs of studies, excluding a total of  `r 347 - sum(!is.na(allData$fis.o) & !is.na(allData$fis.r) & !is.na(allData$n.o) & !is.na(allData$n.r) )` study pairs. See Table 1 for the number of valid studies extracted from each project. 


```{r message=FALSE, warning=FALSE}
# Table 1. A list of each included replication project, the number of replication studies performed as a part of each replication project, the percentage of replication studies which were "successful" (defined here as replication studies which found statistically significant in the same direction as the original study), the number of studies for which are included in the current study, and the percentage of each project's studies which are included in the current analysis.
options(scipen = 1, digits = 3)
# kable(replicationProjects)

# Note: $_a$ Soto et al’s (2018)’s replication rate and was recalculated on the “study” level (i.e., using the number of replicated effects not the number of trait-outcome associations as is reported in the paper) using results disattenuated using the Spearman-Brown prediction formula and Spearman disattenuation formula (Lord & Novick, 1968) to account for less reliable shorter form measures used in the replication studies. $_b$ Klein et al. (2014) includes 4 operationalisations of a single effect which were input separately for analysis in the current study, the bracketed values in the table refer to the number of results at the effect level. 
```


### Analysis

All analyses were performed in R version 3.5.1 {R Development Core Team, 2018 #314} and meta-analyses were performed using the Metafor package version 2.1 {Viechtbauer, 2010 #796} using restricted maximum-likelihood estimation. All analyses and difference scores (i.e., proportion changes and mean differences) were calculated using Fisher z-transformed effect sizes, although effect sizes are back-transformed to correlation coefficients below for easy interpretation unless otherwise stated. All analyses were exploratory, and additional models which were developed and considered are not presented here. See https://github.com/fsingletonthorn/effectSizeAdjustment for a git repository with a record of all interim models and for all model code and data, and see https://osf.io/daj8b for the preregistration of this project, however all reported analyses should be considered exploratory. All of the data and analysis code used in this study, and an RMarkdown document to allow the current paper to be easily reproduced, are available from https://osf.io/7qvna/.

##### Analysis 1: Multilevel random effects meta-analysis 

The first approach uses a random effects meta-analysis framework to estimate the expected effect size difference between original and replication studies.

$difference_{j} = \gamma_0  + u_{id} + u_{article} + u_{project} + e_{j}$

This analysis treats each pair of effects, the original and replicated effect sizes, as one "study" in a meta-analytic framework. This model estimates the change from the original to the replication study effect size ($differnce_{j}$) with a fixed intercept ($\gamma_0$), a random effect for replication project ($\eta_{project}$), a random effect for each original article ($\eta_{article}$), and a random effect for each individual replication ($\eta_{id}$). Random effects at the project level were included to account for non-independence between replications from each replication project. Random effects at the original article level were included to account for cases when multiple effects from an original article were replicated or where multiple operationalisations of an original effect were tested. Standard errors for each difference score were estimated as $$se = \sqrt{\frac{1}{N_{1} -3} + \frac{1}{N_{2} -3} }$$ 
with $N_1$ being the sample size in the original study and $N_2$ being the sample size in the replication study. This standard error is an approximation for the studies that reported F tests with a $df_1$ greater than 1 and chi-squared tests. and in order to check whether this was strongly impacting results all multilevel meta-analyses were re-performed excluding these studies. No differences in the substantive interpretation of results would follow from this change (i.e., the intercept coefficient and random effects variance estimates changed by less than `r round(max( c(validSEDiff, abs(REMod$sigma2 - REModValidSE$sigma2))),3)`).

Using the aggregate summary statistics from the replication projects where a set of labs conducted replications (e.g., the Many Labs Projects) may underestimate the standard error of the difference scores (as their standard error is also a function of the heterogeneity across labs). As a sensitivity analysis, we also ran all multilevel models using a conservative estimate of their sampling variance - calculating their standard errors using the mean sample size included in each replication study as opposed to the total sample size. Again, no differences in the substantive interpretation of results would follow from this change, with the coefficient estimates and estimates of the variance of the random effects changing by less than `r round(max(c(coef(REModAdjusted) - coef(REMod), (REModAdjusted$sigma2) - (REMod$sigma2) ), coef(REModNonequivAdjusted) - coef(REModNonequiv), (REModNonequivAdjusted$sigma2) - (REModNonequiv$sigma2), coef(REModOnlySigAdjusted) - coef(REModOnlySigR), (REModOnlySigAdjusted$sigma2) - (REModOnlySigR$sigma2)), 3)`.

```{r}
options(scipen = 1, digits = 2)
```

#### Accounting for null results

In assessing the degree to which effects are attenuated between original and replication studies it is important to ask how much this effect is driven by the presence of a subset of replication studies where the null hypothesis is true. The average effect size difference between original studies and their replications could be extremely high, and yet this effect could be entirely driven by cases where the null hypothesis is true. For example, if 50% of studies had true null hypotheses, and yet all non-null replication effects were identical to those reported in the original articles, the average attenuation would be 50% despite the fact that the non-null original effect sizes provided unbiased estimates of the replication effect sizes. Analyses 2 to 4 were performed in order to account for this issue.

##### Analysis 2 and 3: Multilevel random effects meta-analysis with exclusions

Analyses 2 and 3 reperform the above meta-analysis excluding studies using two exclusion criteria. Analysis 2 excluded studies in which the replication study was not statistically significant with an effect in the same direction as the original (using the p value reported in the replication projects’ datasets, at an alpha of .05, and using two-tailed tests where applicable). Analysis 3 removed effects in which the replication study effect was “statistically equivalent” to the null according to an equivalence test.

Analysis 2, excluding studies in which the replication study was not significant, means that replication studies with a low level of statistical power to detect the (unknown) true replication effect size are likely to be excluded. This may lead to this analysis underestimating the amount of effect size exaggeration, as replications with non-zero but small effect sizes are likely to be non-significant. This issue is compounded by the fact that some of the replication projects chose the sample sizes that were used in the replication studies using a power analysis of the observed effect in the original study {Open Science Collaboration, 2015 #611}. This approach to designing the replication  studies means that if effect sizes are, on average, smaller in the replication studies than the original reported result, replication studies will often be underpowered.

In order to avoid excluding under-powered studies erroneously, Analysis 3 excluded studies based on whether the replication study results were statistically equivalent to the null hypothesis or statistically significant in the opposite direction {Lakens, 2017 #214;Lakens, 2018 #951}. A requirement for equivalence testing is that an equivalence bound is selected (i.e., an effect size below which the effect size is said to be for all practical purposes equal to zero). For this, we used the lowest effect size that would have been statistically significant in the original study (assuming an alpha of .05), following a suggestion in {Lakens, 2018 #951}. Equivalence tests were performed using Z tests of the Fisher z-transformed effect sizes, excluding studies where the observed replication effect was significantly smaller than the equivalence bound using a one tailed test at the 95% confidence level. Standard errors of each study were estimated as $\sqrt{\frac{1}{N-3}}$, except for studies from {Camerer, 2018 #967} that had more than a single replication attempt. In these cases we used the standard errors derived from the meta-analyses that produced the effect size estimate (see Supplementary Materials 1 for details).

In interpreting results based on this exclusion criterion, it is important to note that the minimum detectable effect was occasionally quite high because original sample sizes were often very small (mean equivalence bound in correlation coefficient terms = `r ztor(mean(minimumEffectDetectableZ,na.rm=T))`, SD =  `r sd(ztor(minimumEffectDetectableZ),na.rm=T)`, 0th, 25th, 50th, 75th and 100th quintiles = [`r quantile(ztor(minimumEffectDetectableZ),na.rm=T)`]). This means that original studies were sometimes under-powered to detect even large effects using the current analysis, and as such this method may exclude studies that have replication effects that the original authors may have considered important {Thompson, 2002 #1039}. See supplementary materials 2 for scatter plots of the dataset using each exclusion rule.

##### Analysis 4: Bayesian mixture model

Analyses 2 and 3 both rely on excluding studies using exclusion rules that will, respectively, exclude or retain studies due to low statistical power in the replication study. In part in order to avoid this issue the final approach to estimating the amount of effect size attenuation conditional on the effect under study being non-zero was the Bayesian mixture model presented in {Camerer, 2018 #967}. This model assumes that each replication effect size comes from one of two components, either from the null-hypothesis or from the alternative-hypothesis component. If the true replication effect size is drawn from the null-hypothesis component, it is assumed to be drawn from a normal distribution with a mean of 0. If the true replication effect size comes from the alternative-hypothesis component, then the it is assumed to be drawn from a normal distribution with a mean equal to the original's estimated true effect size attenuated by an "attenuation factor". The attenuation factor is constrained to a value between zero and one and is assumed to be consistent across studies. The observed replication effect size is then assumed to be drawn from a normal distribution centered on the true replication effect size with a standard deviation equal to the standard error of the replication study (estimated here as $\sqrt{\frac{1}{N_2-3}}$, with $N_2$ being the replication sample size). 

There are two main parameters of interest in this model: (i) the "attenuation factor" (called a "deflation factor" in {Camerer, 2018 #967}), which is the degree to which effect sizes are attenuated between original and replication studies, and (ii) the "assignment rate", which is the overall rate at which studies are assigned to the null hypothesis. This analysis was performed in JAGS version 4.3.0 {Depaoli, 2016 #1010} using the rjags interface (version 4.8.0; {Plummer, 2018 #1011}). See Supplementary Materials 4 for model syntax and further analysis details.

### Descriptives

Looking at the `r tableReductions["Overall", "n included"]` included original-replication study pairs included in this analysis, the effect size in the replication study was lower than that in the original study for `r sum(allData$fisherZDiff < 0, na.rm = T)` articles, `r round(mean(allData$fisherZDiff < 0, na.rm = T)*100)`% of the included studies. An exact binomial test shows that this is extremely unlikely under the assumption that replication effect sizes are equally likely to be smaller or larger in the replication study, p `r if(binom.test(sum(allData$fisherZDiff < 0, na.rm = T), tableReductions["Overall", "n included"])$p.value < .001) "< .001"`. The average effect size for original studies was a Fisher z score equivalent to r = `r tableReductions["Overall", "Mean original ES"]`, and the mean effect size for replication studies was r = `r tableReductions["Overall", "Mean replication ES"]`, a mean decrease of r = `r tableReductions["Overall", "Mean original ES"] - tableReductions["Overall", "Mean replication ES"]`. Notably, this represents an average decrease in effect sizes from the original to the replication study of `r abs(round( tableReductions["Overall", "Mean proportion change"]*100))`%. See Table 2 for a comprehensive list of descriptive statistics on the effect size differences seen in this sample and  Figure 1 for a raincloud plot of the Fisher z-score change in effect sizes by replication project.


```{r out.width = 12, out.height = 20}
knitr::include_graphics("Figures/ViolinPlotZscoreChange.jpg")
```

Figure 1. A raincloud plot (density, box and scatter plot) of the change in effect sizes (here Fisher Z scores) from the original to the replication study by the replication project that each replication study was performed as a part of. 


Table 2. 
Differences between original and replication studies. All calculations were performed on Fisher z-transformed correlations and presented effect sizes are back-transformed into correlation coefficients for interpretability.

```{r, echo = F}
kable( t(tableReductions), digits = 2, col.names = c("All studies", "statistically significant replications", "Nonequivalent studies"))
meanDecrease <- tableReductions$`Mean ES difference`[1]
meanPropDecrease <- tableReductions$`Mean proportion change`[1]
rangeDiffNotOverall <- range(tableReductions$`Mean ES difference`[2:nrow(tableReductions)])
rangePropDiffNotOverall <- range(tableReductions$`Mean proportion change`[2:nrow(tableReductions)])
```


### Results
#### Analysis 1: Multilevel random effects meta-analysis results

The random effects meta-analysis including all data estimated a r = `r REModSum$estimate_Cor` (95% CI [`r REModSum$CI95_Cor`]) decrease in effect sizes from the original to replication studies. This represents a decrease equivalent to `r abs(round( (REModSum$estimate_Z/mean(allData$fis.o, na.rm = T))*100))`% (95% CI [`r abs(round( ((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100)[1]))`%, `r abs(round( ((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100)[2]))`%]) of the mean effect size in the original studies (a Fisher z-transformed correlation coefficient equivalent to a correlation  of `r mean(allData$correlation.o, na.rm = T)`). 

```{r}
options(scipen = 1, digits = 3)
```

Greater variance was attributable to the article and effect level than to the project (with standard deviations for each random effect of $\sigma_{article}$ = `r round(sqrt(REMod$sigma2[2]), 3)`, $\sigma_{effect}$ = `r round(sqrt(REMod$sigma2[3]), 3)`, compared to $\sigma_{project}$ = `r round(sqrt(REMod$sigma2[1]), 3)`), representing an intraclass correlation (ICC) for the project of `r round(REMod$sigma2[1] / sum(REMod$sigma2), 3)`. There was a large amount of unexplained heterogeneity, `r niceREModSum[5, "Random effects"]`, $I^2$ = `r Io2` (calculated following {Nakagawa, 2012 #1023}), suggesting that `r round(Io2)`% of variance in effect sizes was due to heterogeneity (i.e., variance in the true effect size differences), while the remaining `r 100-round(Io2)`%  was attributable to sampling variance. See figure 2 for Empirical Bayes estimates of the random effect for each of the replication projects (calculated following {Raudenbush, 1985 #1055}).

Table 3. Model output from a multilevel random effects meta-analysis of the difference between original and replication effect sizes, with random effects for the project (i.e., which large scale replication project the replication was a part of) and the original (i.e., replicated) article or effect. 

```{r table model summary}
kable(niceREModSum)
```

\n 

```{r blups}
# If you are interested, please uncomment the following for a table of the Empirical Bayes estimates and prediction intervals for random effects (i.e., estimates of the difference between the replication project's mean effect size difference and the overall estimated mean effect size difference). 

 blups <- BLUPsSource[[1]]
 # row.names(blups) <- str_remove_all(row.names(blups), "\\n")
 blups$Project <- row.names(blups)
 blups <- dplyr::select(blups, Project, everything())
 row.names(blups) <- NULL
 # kable(blups, col.names = c("Project" , "Estimate", "Standard Error", "95% PI lower bound", "95% PI upper bound"))

ggplot(blups, aes(x = intrcpt, xmin = intrcpt - se, xmax = intrcpt + se , y = Project)) +  geom_errorbarh() +
  geom_point() + theme_classic() + xlab("Empirical Bayes estimates and standard errors of random effects (Fisher Z score)") + ylab(NULL)

options(scipen = 1, digits = 3)

```

*Figure 2.* Empirical Bayes estimates of the random effect of each replication  project in Fisher Z scores (i.e., estimates of the difference between the replication project's mean effect size difference and the overall estimated mean effect size difference). Error bars are $\pm$ 1 standard error.

#### Analyses 2 and 3: Results from multilevel random effects meta-analysis with exclusions 

Examining just the `r tableReductions["StatisticalSignificance", "n included"]` cases in which the replication study was statistically significant (`r round(tableReductions["StatisticalSignificance", "n included"]/tableReductions["Overall", "n included"],2) * 100`% of all studies), the average effect for the original studies was `r tableReductions["StatisticalSignificance", "Mean original ES"]`, and the mean effect size for replication studies was `r tableReductions["StatisticalSignificance", "Mean replication ES"]`. This represents a mean decrease of r = `r tableReductions["StatisticalSignificance", "Mean original ES"] - tableReductions["StatisticalSignificance", "Mean replication ES"]`, a mean percentage increase in effect sizes of `r abs(round( tableReductions["StatisticalSignificance", "Mean proportion change"]*100))`% and a median percentage decrease of `r abs(round(tableReductions["StatisticalSignificance", "Median proportion change"]*100))`%. Using equivalence testing `r round(( tableReductions["Nonequivalence", "n included"]/ nrow(allData))*100)`% of replication studies were not statistically equivalent to the null (n= `r tableReductions["Nonequivalence", "n included"]`). The average effect size in the original non-equivalent studies was `r tableReductions["Nonequivalence", "Mean original ES"]`, compared to a mean effect size for replication studies of r = `r tableReductions["Nonequivalence", "Mean replication ES"]`. This is a mean decrease of r = `r tableReductions["Nonequivalence", "Mean original ES"] - tableReductions["Nonequivalence", "Mean replication ES"]`, a mean percentage decrease of  `r abs(round(tableReductions["Nonequivalence", "Mean proportion change"]*100))`%,  and a median percentage decrease of `r abs(round(tableReductions["Nonequivalence", "Median proportion change"]*100))`%.

Reperforming the meta-analysis only including studies for which the replication was  statistically significant and had an effect in the same direction as the original produced an estimated r = `r modSumariesR$modelEstimate[2]` (95% CI [`r modSumariesR$MLM95lb[2]`, `r modSumariesR$MLM95ub[2]`]) change in effect sizes from original to replication studies. Including only the studies which were not statistically equivalent leads to a predicted r = `r modSumariesR$modelEstimate[3]` (95% CI [`r modSumariesR$MLM95lb[3]`, `r modSumariesR$MLM95ub[3]`]) decrease in effect sizes. The estimates of the proportion of variance attributable to the article or replication project level did not change considerably in either of these subsets. See Table 4 for the model estimates from each model.

These values represent changes equivalent to a decrease of `r abs(round( ( modRes1$modelEstimate / mean(allData$fis.o, na.rm = T))*100))`% to `r abs(round(( modRes2$modelEstimate / mean(allData$fis.o, na.rm = T))*100))`% of the average original effect size (a correlation coefficient of r = `r ztor(mean(allData$fis.o, na.rm = T))`). However, there was considerable imprecision in these estimates, with 95% confidence intervals for both of these subsamples extending from a considerable decrease equivalent to `r abs(round(( modRes2$MLM95lb  / mean(allData$fis.o, na.rm = T))*100))`% of the average original effect size, to a small increase equivalent to `r round(( modRes1$MLM95ub  / mean(allData$fis.o, na.rm = T))*100)`% of the average original effect size.

##### Table 4
The number of studies included in each model, and the estimated correlation coefficient decrease from each model. 
```{r echo=F}

options(scipen = 1, digits = 2)

colNames <- row.names(modSumariesR) %>% 
        str_replace("Below", " < ") %>%
        str_replace("Above", " > ") %>% 
        str_replace("Significance", "ly significant replications") %>%
        str_replace("Nonequivalence", "Nonequivalent studies") %>%
        str_replace("Overall", "All studies")

rowNames <- c( str_replace(str_replace(names(modSumariesR), "model", "Model "), "MLM95", "95% CI "))

kable(data.frame(rowNames, t(modSumariesR)), col.names = c("Parameter", colNames), row.names = F)

```
Note: Models were estimated using Fisher Z transformed correlation coefficients and back transformed for interpretability. Percentage attenuation gives the percentage attenuation for effect size differences as a percentage of the mean original effect size (r = `r mean(allData$correlation.o, na.rm = T)`). 

##### Leave one out cross validation of meta-analyses

To assess how sensitive the results of the multilevel models were to the inclusion of each of the replication projects, the included studies, and the individual replicated effects, all of the above multilevel models were rerun using leave one out cross validation, excluding both each effect, effects from each original study (i.e., in cases where multiple effects were tested from the same original source), and each replication project one at a time. None of these analyses led to model estimates (i.e., the expected decrease in effect size between original and replication study or equivalently the intercept estimate) that were further than `r LooMaxDiff` from those given above, suggesting that none of the individual projects, effects or studies included were overly influential. See Supplementary Material 3 for tables summarising the leave-one-out model output.

#### Analysis 4: Bayesian mixture model results

The Bayesian mixture model was estimated using four Markov chains from each of which 100,000 draws were taken (excluding an 11,000 draw burn-in period). Trace and density plots for the discussed parameters were examined and, along with and $\hat{R}$ values within .001 of 1, appeared to suggest that the model successfully converged {Gelman, 2011 #1052}. The overall posterior assignment rate (i.e., the proportion of studies that were estimated to be from the non-null alternative hypothesis) was `r round(phiSimple,2)*100`%, with a 95% highest probability density interval of [`r round(phiSimpleHDI, 2 )[1] * 100`%, `r round(phiSimpleHDI, 2 )[2] * 100`%]. The overall attenuation factor (i.e., the estimated amount that effect sizes decreases between the original and replication studies) was `r round((1-as.numeric(alphaSimple))*100)`% with a 95% highest probability density interval of [`r  (1-round(as.numeric(HDISimple)[2], 2))*100`%, `r  (1-round(as.numeric(HDISimple)[1], 2))*100`%]. Figure 2 shows the original effect sizes plotted against replication effect sizes weighted by sample size, along with the posterior assignment rate. The color of each point indicates how often each effect was assigned to the alternative hypothesis.

As was pointed out in {Camerer, 2018 #967}, values close to the diagonal (i.e., cases in which the original and replication effect sizes are similar) were reliably assigned to the alternative hypothesis component whereas effects far below the diagonal were more often assigned to the null hypothesis component. The overall posterior assignment rate might be overly optimistic (i.e., assign studies to the non-null hypothesis at a high rate), likely in part due to the fact that this model allows for "true" effect sizes to be estimated as being extremely low or near zero and still assigned to the alternative hypothesis, with `r round(propBelow.1.BMM, 2) * 100`% of the estimated "true" replication effect sizes being smaller than a correlation coefficient of .10.

```{r,  dpi=300}
mixtureModelPlot + coord_equal(ratio=1)
```

###### Figure 3. 
A scatterplot of replication study effect sizes (in correlation coefficients) plotted against original study effect sizes, colored by the posterior assignment rate, the proportion of times each study was assigned to the alternative hypothesis. Points that fall on the solid, diagonal line represent replication effect sizes equal to the original effect sizes. Point size represents (the log) of the number of participants in the replication study.

## Discussion 

The results show that there was a substantial average decrease in effects sizes between original and replication studies and suggest that this is still the case even after accounting for the presence of null effects. The results of the multilevel meta-analysis show an estimated mean decrease of r = `r REModSum$estimate_Cor`, (95% CI [`r REModSum$CI95_Cor`]), equivalent to a `r REModSum$estimate_D` point Cohen's d decrease (95% CI [`r REModSum$CI95_d`]), or an estimated decrease of `r abs(round((REModSum$estimate_Z/mean(allData$fis.o, na.rm = T)),2))*100`%  (95% CI [`r abs(round((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100))[1]`%, `r abs(round((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100)[2])`%]) of the mean effect size in the original studies (a Fisher z equivalent to r = `r ztor(mean(allData$fis.o, na.rm = T))`). 

Arguably of more interest to researchers examining and planning research is the question of how much effect size attenuation should be expected under the assumption that the effect size is non-zero. All of the methods used here largely agreed, although the degree of precision in their estimates differs. The Bayesian mixture model suggests that there is an average decrease of `r round((1-as.numeric(alphaSimple))*100)`%, with a 95% highest probability density interval of [`r  (1-round(as.numeric(HDISimple)[2], 2))*100`%, `r  (1-round(as.numeric(HDISimple)[1], 2))*100`%]. The multilevel meta-analysis models that excluded non-significant results and studies in which the replications studies were statistically equivalent to the null both led to similar conclusions, although they give less precise estimates, highlighting the uncertainty in these estimates. For example, the confidence intervals over both of the models in Analyses 2 and 3 extend from a meaningful decrease of r = `r ztor(min(modSumariesR$MLM95lb[-1]))` to a trivial  increase of r = `r ztor(max(modSumariesR$MLM95ub[-1]))`.

In using these results to inform future research (e.g., in sample size planning) and to interpret the published literature, it is essential to take note of the level of heterogeneity in the effect size attenuation across not just replicated effects but also across replication projects. The sampling decisions and replication methods used by each of the included projects appears to have had a considerable effect on the amount of effect size attenuation seen (e.g., looking at the results of Analysis 1, the estimated standard deviation of the  mean level of effect size attenuation  across projects is `r round(sqrt(REMod$sigma2[2]), 3)`, 95% CI [`r confIntsREMod[[1]]$random[2,c(2,3)]`]). The degree of heterogeneity in the amount of effect size attenuation across studies and projects means that using any single estimate of the amount of effect size decrease is likely to be misleading in the case of any individual replication study.

```{r, dpi=300}
catPlot 
```

*Figure 4.* A caterpillar plot of the effect size differences between original and replication study effect sizes ordered by magnitude in Fisher Z score units, error bars are 95% confidence intervals around effect size differences.

```{r dpi = 300, warning=F}
grid.arrange(project_title, pannel1,
             article_title, pannel2,
             effect_title, pannel3,
             heights=c(1.6,9,1.6,9,1.6,15), ncol = 1)
```

*Figure 5.* Histograms of the expected differences (i.e., the sum of the Empirical Bayes estimates for random effects and the model intercept from Analysis 1) between original and replication studies for each replication project (top panel), study (middle panel), and effect (bottom panel) in Fisher Z score, illustrating the degree of heterogeneity expected at each level of the multilevel model.

#### Limitations and future directions

In interpreting these results it is important to note several limitations. Firstly, the current study cannot distinguish between effect size *heterogeneity* (i.e., effect sizes that are different due to subtle unobserved moderators {Kenny, 2019 #1041} and effect size *exaggeration*. However, in so far as effect size heterogeneity tends to lead to smaller effects in replication studies, the distinction may not matter for the purposes of researchers hoping to replicate or plan future similar studies of the same type of effects. It also cannot be ruled out that the effect size differences seen in these large-scale replication projects are larger than would be seen by individuals attempting to replicate particular effects (e.g., if researchers in these large-scale replications have less access to the tacit knowledge that would normally facilitate replicators' efforts).

The Bayesian mixture model assumes independence between effects, a uniform attenuation factor across all areas of psychological research, and allows for effects sampled from the alternative distribution to be negligibly small or even negative. Future research could help develop a more nuanced account of the data-generation process underlying this dataset by, for example, building a model that allows for the attenuation rate to change across replication studies, or by including more components in order to allowing for studies with negligible or negative but non-null effects in addition to the alternative and null components.

The large amount of heterogeneity across replication projects limits the utility of the fixed effects parameter estimates (i.e., the estimated mean level of attenuation seen across all projects). While the current dataset allows us to make reasonable inferences about future replication projects, the varied sampling strategies employed by the previous projects necessarily limit the inferences we can draw. At the moment, it is difficult to say whether the heterogeneity in effect size attenuation are due to intra-field differences in replicability and effect size attenuation, differences caused by the sampling strategies, or other issues such as differences in the quality of the replication studies.

In order to be able to start to disentangle these different possible causes and develop an account of the predictors and moderators of replication success and effect size attenuation, we need to begin to develop a large representative database of replication studies. Especially if such a database could be augmented with meta-data regarding the type of analysis, design and effects under study, this resource could allow us to make meaningful predictions about individual future replications. However, until a large database of such studies becomes available, analyses like the current one provide the best estimates possible, albeit estimates that should be read and understood with these caveats in mind.

### Conclusion 

The findings of this study reinforce the importance of recent efforts to reduce psychology literature’s reliance on underpowered original research designs, to circumvent publication bias, and to avoid QRPs like p-hacking and HARKing {Bakker, 2012 #38}. Efforts to avoid the impact of any of these issues would likely reduce the degree to which effect sizes are attenuated in replications of the primary research literature. 

In order to avoid performing future underpowered research, researchers should be aware that their experiments are likely to be underpowered if they plan their sample sizes using the effect size reported in a previous experiment. As a conservative heuristic for researchers performing formal sample size planning on the basis of previous research, researchers could follow the advice given in {Camerer, 2018 #967} and plan their experiments assuming that the true effect size is 50% of the reported effect size, a value matched by the more extreme 95% confidence interval of the estimated amount of effect size exaggeration across studies in this sample. Alternatively, it may be preferable to use methods of sample size planning that do not rely on precise a priori estimation of the effect size under study, such as planning studies to reliably detect the smallest effect size of interest {Lakens, 2018 #951}, using sequential analysis strategies {Pocock, 1977 #553}, or planning for adequate precision in parameter estimates across a range of possible effect sizes {Maxwell, 2008 #559;Kelley, 2017 #727}. In addition, recent large-scale multinational data collection efforts like the Many labs Projects or the Psychological Science Accelerator {Moshontz, 2018 #1025} also help to avoid the negative impacts of low statistical power by allowing for extremely high powered studies of even very small effects.

There are several recent efforts to reduce the impact of publication and reporting biases that readers should be aware of, many of which individual researchers can voluntarily and easily take part in. Careful preregistration of analysis plans allows researchers to avoid biases in the analysis of their data that may otherwise lead to inflated effect sizes {Wicherts, 2016 #475}. Data-sharing platforms such as figshare (figshare.com) and the Open Science Framework (osf.io) make it possible for researchers to easily share the results of research whether or not a study is published in a traditional journal. Similarly, pre-prints (e.g., https://psyarxiv.com) allow researchers to report and publicize reports and data that may otherwise remain in the file draw. Both preprints and data repositories make it easier to ensure that non-significant results are accessible to other researchers and meta-analysts. Finally, registered reports, in which papers are reviewed before data-collection on the basis of the research design and analysis strategy as opposed to the results, also show promise in helping to develop a body of literature that is not affected by reporting and publication bias {Nosek, 2014 #202}. However, until large bodies of research free of publication bias become available, researchers should be aware that effect sizes in published studies may be considerably overstated.


## Supplementary material
## SM1
### Replication project sampling frames, extraction, participants, and exclusion details

#### {Open Science Collaboration, 2015 #611}
##### Sampling frame
{Open Science Collaboration, 2015 #611} performed a total of 100 replications. They attempted to obtain a quasi-random sample by sampling from articles published in 2008 from the journals Psychological Science, Journal of Personality and Social Psychology, and Journal of Experimental Psychology: Learning, Memory, and Cognition. Replication teams were instructed to select from a subset of the sampling frame (initially just the first 20 published articles from each journal were available, a pool that was expanded as it became difficult to match teams with articles). Project coordinators also actively recruited replication teams from the community that they thought had expertise to assess particular claims. Of the 488 articles that were published in the applicable journal issues, 158 (32%) were eligible for selection. A total of 84 of the 100 replications conducted were of the final experiment in each original article. Sixteen replications deviated from this rule and used a different experiment due to a recommendation from the original authors or on the basis of feasibility. The statistical test that was chosen to reflect the replication outcome was presented to the original authors as part of the design protocol, and was changed at the request of the original authors.

Open Science Collaboration (2015) reports that 47 articles were eligible for replication and were not claimed. Six of these studies were deemed infeasible due to practical constraints such as cost, historical dependence, or difficult to recruit samples, and were not presented to potential replicators. The remaining 41 were presented to potential replicators, but were not claimed. Open Science Collaboration (2015) reports that these projects "often" required access to rare populations, were likely to be expensive, required specialized materials (e.g., fMRI studies), or access to specialized knowledge.


```{r}
studyNs <- allData$n.r[allData$abrev == "OSCRPP"]
```

##### Participants
Each replication study was powered on the basis of the original study's observed effect size. The average power to detect the original effect of the 97 studies with significant results was .92. Individual studies had sample sizes (at the individual level) of between `r min(studyNs)` and `r max(studyNs)` (mean n = `r mean(studyNs)`, median = `r median(studyNs)`, sd = `r sd(studyNs)`).

##### Extraction and inclusion rules
Three original studies which did not report that their findings were indicative of a non-zero effect were excluded from those studies extracted from {Open Science Collaboration, 2015 #611}. Three studies for which z transformed correlation coefficients could not be extracted due to missing data in the downloaded data set were also excluded from analysis (these included 1 study which used multiple statistical tests in the original and replication studies, one study in which the replication and original study used different statistical tests, and one study for which the effect size was reported as a beta coefficients without test statistics or degrees of freedom). Effect sizes for original and replication studies are included for `r nrow(data)` out of 97 studies replicated studies from {Open Science Collaboration, 2015 #611} which reported having found a non-zero effect.

#### {Camerer, 2018 #967}
##### Sampling frame and reported deviations

```{r}
studyNs <- allData$n.r[allData$abrev == "natSci"]
```

Camerer et al. (2018) replicated all 21 studies that were (a) published between 2010-2015, (b) in the journals Nature or Science,  (c) tested for an experimental treatment effect, (d) had "at least one clear hypothesis with a statistically significant finding", and that (e) "were performed on students or other accessible subject pools". In order to choose which of the original studies’ statistical tests to use as the replication outcome, they selected the statistical tests that they subjectively identified as the most important in the first reported study that reported a significant treatment effect. When they could not identify a single most important effect they randomly selected from those that they identified as potential candidates. Camerer et al. (2018) report that there were no deviations from original studies’ protocols for seven replications, “minor” deviations for 12, and an “unintended” deviation for one study.

##### Participants

Camerer et al. (2018) took a two stage sampling procedure, where each study was initially replicated  with a sample large enough to ensure 90% power to detect an effect 75% of the size of the original study's observed effect size. If there were no significant results in this first study, a second study was performed with 90% power to detect 50% of the original effect size. For one study, there was a significant result in the same direction as the original study in the first wave of replication data collection, but a second wave was still performed due to an error. 

##### Extraction and inclusion rules

Original and replication effect sizes were extracted for all `r nrow(data4)` studies included in {Camerer, 2018 #967}. In some cases in the Nature Science reproducibility project {Camerer, 2018 #967} multiple replication studies were performed for a single effect. In each of these cases we performed a fixed effects meta-analysis using the metafor package {Viechtbauer, 2010 #796} to estimate a meta-analytic effect size estimate. The effect size, standard errors and sample sizes used in the current study reflect this pooled estimate. This method leads to one study more "replicating" according to the 'statistical significance in the same direction of the original study' criterion than was originally reported in {Camerer, 2018 #967}, where they used the p value from the largest performed study instead of a pooled estimate.

#### {Soto, in press #1032} 

##### Sampling frame
Soto (2019) extracted 78 trait-outcome associations (i.e., associations between The Big Five personality traits and life outcomes like volunteerism or politically conservatism) from a review article that presented a total of 86 associations between the Big Five traits and life outcomes (Ozer & Benet-Martinez, 2006). Soto and a research assistant extracted the empirical evidence behind these associations, and selected the 78 that were replicated on the basis of feasibility. In Soto (2019) the 78 trait-outcome associations were the primary units of analysis for estimating replicability, whereas in the current study we used the study level for comparability with the other replication projects. In total `r nrow(loopr)` studies were included in the current analysis.

##### Deviations from original study protocols
All replications were surveys administered using Qualtrics. The replications were all conducted in one of two large surveys. A single measure of the Big 5 personality traits was administered rather than the disparate versions used in each individual original study. The outcomes were assessed using measures that Soto reports were "selected to follow the original studies as closely as was feasible". Soto reports that most outcome measures were administered using the same measure, but some interview questions were altered to fit a survey format and some lengthy questions or measures were shortened. As this means that some replication studies used shorter form versions of the original data collection instruments, all significance tests or effect size analyses in the current paper use the effect sizes that Soto (2019) reports have been disattenuated using the Spearman-Brown prediction formula and Spearman disattenuation formula to estimate the trait-outcome associations that would be expected if the measures had used the same number of items as the original study (Lord & Novick, 1968).

##### Participants
Two young adult (18-25 year olds) and two adult samples (18 years of age or older) were recruited using quota sampling. Quota sampling was employed in order to match the US population in terms of sex, race and ethnicity in both the adult  and young adult samples, and additionally for age, educational attainment and household income in the adult samples. Samples of 1559 adults and 1550 young adults completed Survey 1 (including approximately half of the replication studies), and samples of 1512 adults and 1505 young adults completed Survey 2 (which included the remaining replication studies). Soto reports that this yielded a minimum power of 97.3% to detect a  correlation of just .1 using a two-tailed test with an alpha of .05.

##### Extraction and inclusion rules
Effect sizes were extracted for original and replication studies for `r sum(!is.na(data7$correlation.o) & !is.na(data7$correlation.r))` out of `r nrow(loopr)` included studies, and one original study's sample size was not available. In {Soto, in press #1032} effect sizes which were only reported in this dataset as beta coefficients were not converted to Fisher z scores as not enough information was available in  Soto's data set to do so. A total of `r sum(allData$abrev == "loopr")` of 121 studies were included in the current analysis. Following the other large scale replication studies, the signs of the original and replication study effects were inverted for analysis if the original effect was negative. 

#### Experimental philosophy replication project {Cova, 2018 #984}

##### Sampling frame and deviations from original protocols
Cova et al. (2018) sampled their replication targets from a list of experimental philosophy papers hosted by Yale's University's Experimental Philosophy Lab  (http://experimental-philosophy.yale.edu/ExperimentalPhilosophy.html), and filtered these down to articles published in one of 35 scientific journals that Cova and Strickland judged would likely have published experimental philosophy research. From the remaining articles, they sampled three papers per year for the years 2003 to 2015, selecting the most cited paper (according to Google scholar) and two random studies (barring the year 2003 where there were only two applicable papers). This left them with a list of 38 studies which were assessed for feasibility. They report that four studies were identified as being potentially difficult to replicate (as they required access to special populations, three papers requiring access to non-English speakers and one requiring participants with high-functioning autism). In order to ensure that they achieved at least their goal number of studies, they sampled an additional replacement for each of the original studies they were concerned might be practically infeasible (either picking the second most cited article of the year if the potentially infeasible study was selected as it was the most cited, or an additional random article), but continued to attempt to replicate all studies. In the end, two of the studies that were identified as being potentially infeasible were replicated, and two were not. The "more feasible" replication studies were also included in the final analysis if they were finished by the end of the project's duration. This left a total of 40 completed replications, 12 replications of most the most cited article of the years in the sampling frame, two replications of the second most cited article in a given year, and 26 studies which were randomly chosen from each year's set of experimental philosophy studies.

Cova et al. (2018) report that there was a major deviation in protocol for one study, where text was presented on screen for a set duration instead of being removed after participants responded, which they suggest may have reduced participants' incentive to quickly respond.

##### Participants
The research teams were based in Brazil, France, Lithuania, Netherlands, Spain, Switzerland, United Kingdom and the United States. Most studies were replicated using online samples (35 studies) and four used university students, unlike the original studies of which 25 used collage samples, 10 used online samples, and four involved local pedestrians. Cova et al. (2018) report that their average replication sample size was 206.3, sd = 131.8. For the 32 studies reporting a significant result for which they could perform a power analysis, the average level of statistical power to detect the original effect size was  0.88 (SD = 0.14), 26 had a power > 0.80 to detect the original sample size, and 18 had a power > .95. 

##### Extraction and inclusion rules
{Cova, 2018 #984} included three replications of original studies which were non-significant (and which did not claim to provide evidence for the effects under test), these were removed from this analysis. Effect sizes were reported by Cova et al. (2018) and are included in the current study for `r sum(!is.na(data6$fis.o) & !is.na(data6$fis.r) & !is.na(data6$n.o) & !is.na(data6$n.r))`, original and replication studies, out of an original 37 replicated studies with significant original results. The four studies for which no effect sizes were reported performed analyses for which Cova et al. (2018) could not develop reasonable effect size estimates from the reported statistics or data (e.g., a Sobel test, GEE analysis).

#### Many labs 1 {Klein, 2014 #988}
##### Sampling frame
Many Labs 1  {Klein, 2014 #988} replicated a non-random sample of 13 effects from 12 original articles. They report that the replicated studies were chosen on the basis of being (a) suitable for online presentation, (b) able to be presented "quickly" and (c) to have a simple, two condition design (except for a single a correlational study which was included). The replication materials were translated into the local language of each sample.

##### Participants
Klein et al. (2014) replicated each effect across 36 samples and settings (27 laboratory, 9 online, and 25 in the USA and 11 in non-US locations) including a total of 6344 participants. The individual studies each included between 4896 and 6336 participants.

##### Extraction and inclusion rules
One of the original studies in Many labs 1 {Klein, 2014 #988} did not report an effect size or test statistic so is not included in the current sample. No effect size was extractable for one original study, and this effect was excluded for the purposes of the current analysis. Four different operationalisations of anchoring effects were tested, all of which are included in the current analysis, leading to a total of 15 paired data-points being included from this study. The multilevel models reported account for non-independence between these effects.

#### Many labs 2 {Klein, 2018 #1021}

##### Sampling frame
Many labs 2 {Klein, 2018 #1021} replicated a non-random sample of 28 effects. In order to select the claims that were replicated, they (a) held an open nomination round that fit a set of desired characteristics, elicited ideas within the project team, and asked "independent experts" who worked in the field of psychological science. The criteria for nominations were that the study must "(a) have procedures that can be implemented via a web browser, (b) not have more than two conditions in an experimental design, (c) have one outcome (dependent variable), (d) be correlational or experimental designs, though the latter is preferred, and (e) examine a psychological topic". There were also a set of desired characteristics, that the study should: "(a) be “important” in at least one of the many variety of ways of demonstrating importance, (b) be brief, (c) be a direct replication of an existing published design from which an original effect size can be determined)" (see https://osf.io/uazdm/ for the coordinating proposal).

After over 100 claims were nominated, the Klein et al., chose the replicated claims on the basis of the claims being (a) suitable for online presentation, (b) able to be presented quickly, (c) citation count (more being judged more positively), (d) to have a simple, two condition design or, at a lower priority, a correlational design, (e) to have a high level of "general interest", and (f) to be applicable to a general sample of adults. Secondarily, Klein et al., report that they aimed to ensure a diversity of claims, including (a) claims that had both limited and extensive literatures across samples and settings, (b) effects for which it was expected that there would be high and low effect size heterogeneity, (c) "classic" and "contemporary" effects, (d) effects which cover a broad range of subareas within social and cognitive psychology, (e) effects from a diversity of research teams, and finally (f), effects from a variety of journals. 

For all chosen claims, the original study's corresponding author were contacted (if possible) and asked for the original materials, and asked if there were any limitations or moderators that the replication teams should be aware of before attempting the replication. Klein et al., report that this process eliminated some studies, at which point the effect was eliminated from consideration and replaced with another of the nominated claim or the original study's authors were asked to help identify alternative studies for replication. In one case, the original authors expressed "strong concerns" about the study being included in this project, and this claim was removed. 

After this process, 31 effects remained, which were split into two 30 minutes "slates", and pilot tested. Three effects were removed for length, and some studies were shortened in order to fit into the two 30 minutes slates after pilot testing. This left 28 effects that were replicated. 


##### Participants
```{r echo = F}
studyNs <- c(7460, 7218, 7901, 5882, 6608, 7117, 6826, 7890, 3549, 7001, 6506, 6234, 6935, 4204, 6905, 7197, 7982, 6842, 7205, 8002, 7827, 7396, 7646, 7228, 6591, 6966, 7923, 8000)
```

Labs were invited to participate in an open call made in 2014. The only criteria for inclusion was that a lab had to agree to include at least 80 participants. A total of 125 samples were recruited in this study. The total sample size was 15305, with ns of at least `r min(studyNs)` in each replication study after exclusions (mean n = `r mean(studyNs)`, sd = `r sd(studyNs)`, median = `r median(studyNs)`)

##### Extraction and inclusion rules
A total of `r nrow(data8)` of `r nrow(data8) + 6 ` paired original and replication effects sizes are included for this analysis. Four studies from {Klein, 2018 #1021}  were removed because the original and replication studies examined a difference in effect sizes seen in different conditions, and the effects were not directly tested against each other making it difficult to derive an appropriate effect size. Two additional studies were excluded because their effect sizes were only available as Cohen's q.

#### Many labs 3 {Ebersole, 2016 #985}
##### Sampling frame
A non-random sample of nine original studies were replicated, and one conceptual replication was performed. The conceptual replication was excluded from the current analyses. Ebersole et al., report that they aimed to identify two-condition experiments or correlational designs. Seven out of the nine included studies were included in a single computer delivered experiment, and were followed by extensive individual difference and data quality indicators. Two experiments were exclusively performed in person as, in one case, the study required participants to physically hold a clipboard, and in one as the original authors said that the experiment needed to be administered using a paper and pencil format. 

##### Participants
```{r}
studyNs <-  c(3337, 1335, 3088, 2969, 2285, 3119, 2365, 3136, 3134)
```

The replication included 20 university participant pools at universities in the United states and Canada (N = 2,696) and an mTurk online sample (N = 737). Individual studies had ns of at least `r min(studyNs)` after exclusions (mean n = `r mean(studyNs)`, sd = `r sd(studyNs)`, median = `r median(studyNs)`)

##### Extraction and inclusion rules
Original and replication effect sizes were extracted for all 9 original and replication studies from {Ebersole, 2016 #985}, excluding a study they term a "conceptual replication". Most effects (6/9) were converted to correlation coefficients from the Cohen's d values reported in this replication project. The results of three additional studies reported as partial Eta squared were converted to correlation coefficients from F statistics using the formula: 

$r\  =\ \sqrt{\frac{F_{obs}\times(df_1 / df_2)}{F_{obs} \times (df_1 / df_2) + 1}} \times \sqrt{\frac{1}{df_1}}$

#### Economics replication project {Camerer, 2016 #983}
##### Sampling frame and deviations
Camerer et al. (2016) replicated all 18 between-subject laboratory experimental papers published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. For 17 of 18 studies, the same software was used to conduct the replications as was used in the original experiment. In one study the software used was different as the original program was no longer available.

##### Participants
```{r}
studyNs <- allData$n.r[allData$abrev == "econ"]
```

Two American teams, one Swedish, and one Austrian replication team conducted four or five replications each. Camerer (2016) reports that the replication sample sizes were chosen to ensure that each replication study had at least 90% power to detect the original study's effect size using the original analysis strategy. Individual studies had ns of at least `r min(studyNs)` (mean n = `r mean(studyNs)`, sd = `r sd(studyNs)`, median = `r median(studyNs)`). 

##### Extraction and inclusion rules
The economics replication project {Camerer, 2016 #983}. Original and replication effect sizes for all `r sum(allData$abrev == 'econ')` studies were reported in correlation coefficients and all are included in this analysis.

### SM2 
#### Plots and multilevel model output of the relationship between original and replication correlation coefficients using varied exclusion criteria

The following output shows scatter plots and model output for all of the multilevel meta-analyses performed using the varied exclusion criteria explained in the main text.

Table SM `r tabSMN <- 1; tabSMN`. Multilevel meta-analysis model estimates and random effects for all data.

```{r}
kable(niceModelSums$`All Data`)
```

```{r}
plotAllData 
```

Figure SM`r figSMN <- 1; figSMN`. Scatter plot of replication effect sizes (in correlation coefficients) plotted against original effects including all data.


Table SM`r tabSMN <- tabSMN + 1; tabSMN`. Multilevel meta-analysis model estimates and random effects including only statistically significant replications. 

```{r}
kable(niceModelSums$`Only Signficant replications`)
```


```{r}
plotSigR 
```

Figure SM`r figSMN <- figSMN+1; figSMN`. Scatter plot of replication effect sizes (in correlation coefficients) plotted against original effects including only statistically significant replications. 


Table SM`r tabSMN <- tabSMN + 1; tabSMN`. Multilevel meta-analysis model estimates and random effects including studies which are not statistically equivalent to the null, using equivalence bounds set as the minimum effect size that would have been statistically significant in the original study. 

```{r}
kable(niceModelSums$`Non-equivalent studies`)
```

```{r}
plotNonequiv 
```

Figure SM`r figSMN <- figSMN+1; figSMN`. Scatter plot of replication effect sizes (in correlation coefficients) plotted against original effects including studies which are not statistically equivalent to the null, using equivalence bounds set as the minimum effect size that would have been statistically significant in the original study.


### SM3
#### LOO Cross validation output

#### Table [LOO cross validation output].
0th, 25th, 50th, 75th and 100th percentiles from leave one out cross validation for each multilevel model, for each exclusion method an, including only the sample indicated in "LOO exclusions". 

```{r}
kable(LOOoutput)
```

### SM4
#### Bayesian Mixture Model

The mixture model results presented in text presents the model developed by Camerer et al,. (2018; see https://osf.io/xhj4d/ for their detailed description of this model).  All priors were chosen to be uninformative or vague. The mixture model assumes that the observed replication effect sizes either come from the null hypothesis, a true effect sampled from a normal distribution with a mean of zero and an estimated precision (tau). This model uses an errors-in-variables approach to account for possible attenuation of effect sizes due to measurement error and estimation uncertainty following {Matzke, 2017 #1012}, which means the effect size attenuation factor is the factor change between the estimated true effect of the original and replication study effect size.

Box SM1. The original model reported in {Camerer, 2018 #967} and reported on in the main text of the current article. 

```{}
model{
# Mixture Model Priors:
alpha ~ dunif(0,1) # flat prior on slope for predicted effect size under H1
tau ~ dgamma(0.001,0.001) # vague prior on study precision
phi ~ dbeta(1, 1) # flat prior on the true effect rate
# prior on true effect size of original studies:
for (i in 1:n){
trueOrgEffect[i] ~ dnorm(0, 1)
}
# Mixture Model Likelihood:
for(i in 1:n){
clust[i] ~ dbern(phi)
# extract errors in variables (FT stands for Fisher-transformed):
orgEffect_FT[i] ~ dnorm(trueOrgEffect[i], orgTau[i])
repEffect_FT[i] ~ dnorm(trueRepEffect[i], repTau[i])
trueRepEffect[i] ~ dnorm(mu[i], tau)
# if clust[i] = 0 then H0 is true; if clust[i] = 1 then H1 is true and
# the replication effect is a function of the original effect:
mu[i] <- alpha * trueOrgEffect[i] * equals(clust[i], 1)
# when clust[i] = 0, then mu[i] = 0;
# when clust[i] = 1, then mu[i] = alpha * trueOrgEffect[i]
  }
}
```


### SM5

#### Conversions

All statistical tests extracted were transformed into correlation coefficients as follows, using the methods reported in {Open Science Collaboration, 2015 #611}. 

t statistics:

$r\ =\ \sqrt{\frac{t_{obs}^2\times(1 / df)}{(t_{obs}^2 / df) + 1}}$

Where $t_{obs}$ is the observed t statistic and $df$ is the degrees of freedom of the t test. 

F statistics:

$r\  =\ \sqrt{\frac{F_{obs}\times(df_1 / df_2)}{F_{obs} \times (df_1 / df_2) + 1}} \times \sqrt{\frac{1}{df_1}}$

Where $F_{obs}$ is the observed F statistic and $df_1$ is the degrees of freedom of the numerator and $df_2$ is degrees of freedom of the denominator.  

Chi square statistics:

$r\ =\ \sqrt{\frac{\chi^2_{obs}}{df + 2}}$

Where $\chi^2_{obs}$ is the observed $\chi^2_{obs}$ statistic and $df$ is the associated degrees of freedom.

All values were then transformed into fisher Z transformed correlation coefficients using:

$z\ = \frac 12 \times \ln{\Big(\frac{1 + r}{1 - r}}\Big)$

