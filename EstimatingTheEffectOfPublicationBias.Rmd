---
title: "EstimatingPublicationBias"
output:
  word_document
---

```{r message=FALSE, warning=FALSE, include=FALSE}
source(file = 'Data/data_collection_cleaning.R')
source(file = 'simplifiedAnalysisScript.R')
options(scipen = 1, digits = 2)
knitr::opts_chunk$set(fig.width=8, fig.height=5, echo =FALSE) 
options(knitr.kable.NA = '')
```

## Introduction
See manuscript

## Methods 
### Data extraction
All eight published or in press large scale replication projects performed within in the behavioral science research literature were included in the current research (see Table 1 for a list of the included studies). The source of each replicated effect, reported test statistics, effect sizes, sample sizes, and p-values were extracted for each original and replication study. Several of the large scale replication projects did not present the original test statistics and p values (e.g., Many Labs 1 and 3 {Klein, 2014 #988;Ebersole, 2016 #985}). In these cases, these values were manually extracted from the original articles. When sample sizes for original studies were not reported in the data provided by each replication project they were manually extracted from original articles where possible.

For all analyses, the original and replication effect sizes were transformed to Fisher z-transformed correlation coefficients following the methods used in Open Science Collaboration (2015, see Supplementary Materials 5 for details). This conversion used data from the replication project whenever possible (i.e., whenever effect sizes were reported in correlation coefficients in a summary table or in a project’s online data this was directly converted to Fisher z values). If the study-level results were not reported as correlation coefficients, Cohen’s d values, as t-tests, or as F statistics in the original or replication project we excluded the result from this analysis (e.g., cases when no effect size was reported in the original study or in the replication project data set). In cases where sample sizes were not reported per group, sample sizes among groups were assumed to be equal in these conversions. For each of the Many Labs projects the top level result was used (i.e., the results of the analysis that collapsed the data across the multiple labs). See Supplementary Materials 1 for a comprehensive account of exclusions and study specific extraction details for each replication project. An original and replication effect size that could be converted to a Fisher z-score, along with sample sizes for original and replication studies, was extracted for a total of `r sum(!is.na(allData$fis.o) & !is.na(allData$fis.r) & !is.na(allData$n.o) & !is.na(allData$n.r) )` pairs of studies, excluding a total of  `r 347 - sum(!is.na(allData$fis.o) & !is.na(allData$fis.r) & !is.na(allData$n.o) & !is.na(allData$n.r) )` study pairs. See Table 1 for the number of valid studies extracted from each project. 


```{r message=FALSE, warning=FALSE}
# Table 1. A list of each included replication project, the number of replication studies performed as a part of each replication project, the percentage of replication studies which were "successful" (defined here as replication studies which found statistically significant in the same direction as the original study), the number of studies for which are included in the current study, and the percentage of each project's studies which are included in the current analysis.
options(scipen = 1, digits = 3)
# kable(replicationProjects)

# Note: $_a$ Soto et al’s (2018)’s replication rate and was recalculated on the “study” level (i.e., using the number of replicated effects not the number of trait-outcome associations as is reported in the paper) using results disattenuated using the Spearman-Brown prediction formula and Spearman disattenuation formula (Lord & Novick, 1968) to account for less reliable shorter form measures used in the replication studies. $_b$ Klein et al. (2014) includes 4 operationalisations of a single effect which were input separately for analysis in the current study, the bracketed values in the table refer to the number of results at the effect level. 
```


### Analysis

All analyses were performed in R version 3.5.1 {R Development Core Team, 2018 #314} and meta-analyses were performed using the Metafor package version 2.1 {Viechtbauer, 2010 #796} using restricted maximum-likelihood estimation. All analyses and difference scores (i.e., proportion changes and mean differences) were calculated using Fisher z-transformed effect sizes, although effect sizes are back-transformed to correlation coefficients below for easy interpretation unless otherwise stated. All analyses were exploratory, and additional models which were developed and considered are not presented here. See https://github.com/fsingletonthorn/effectSizeAdjustment for a git repository with a record of all interim models and for all model code and data, and see https://osf.io/daj8b for the preregistration of this project, however all reported analyses should be considered exploratory. All of the data and analysis code used in this study, and an RMarkdown document to allow the current paper to be easily reproduced, are available from https://osf.io/7qvna/.

##### Analysis 1: Multilevel random effects meta-analysis 

The first approach uses a random effects meta-analysis framework to estimate the expected effect size difference between original and replication studies.

$difference_{j} = \gamma_0  + u_{id} + u_{article} + u_{project} + e_{j}$

This analysis treats each pair of effects, the original and replicated effect sizes, as one "study" in a meta-analytic framework. This model estimates the change from the original to the replication study effect size ($differnce_{j}$) with a fixed intercept ($\gamma_0$), a random effect for replication project ($\eta_{project}$), a random effect for each original article ($\eta_{article}$), and a random effect for each individual replication ($\eta_{id}$). Random effects at the project level were included to account for non-independence between replications from each replication project. Random effects at the original article level were included to account for cases when multiple effects from an original article were replicated or where multiple operationalisations of an original effect were tested. Standard errors for each difference score were estimated as $$se = \sqrt{\frac{1}{N_{1} -3} + \frac{1}{N_{2} -3} }$$ 
with $N_1$ being the sample size in the original study and $N_2$ being the sample size in the replication study. This standard error is an approximation for the studies that reported F tests with a $df_1$ greater than 1 and chi-squared tests. and in order to check whether this was strongly impacting results all multilevel meta-analyses were re-performed excluding these studies. No differences in the substantive interpretation of results would follow from this change (i.e., the intercept coefficient and random effects variance estimates changed by less than `r round(max( c(validSEDiff, abs(REMod$sigma2 - REModValidSE$sigma2))),3)`).

Using the aggregate summary statistics from the replication projects where a set of labs conducted replications (e.g., the Many Labs Projects) may underestimate the standard error of the difference scores (as their standard error is also a function of the heterogeneity across labs). As a sensitivity analysis, we also ran all multilevel models using a conservative estimate of their sampling variance - calculating their standard errors using the mean sample size included in each replication study as opposed to the total sample size. Again, no differences in the substantive interpretation of results would follow from this change, with the coefficient estimates and estimates of the variance of the random effects changing by less than `r round(max(c(coef(REModAdjusted) - coef(REMod), (REModAdjusted$sigma2) - (REMod$sigma2) ), coef(REModNonequivAdjusted) - coef(REModNonequiv), (REModNonequivAdjusted$sigma2) - (REModNonequiv$sigma2), coef(REModOnlySigAdjusted) - coef(REModOnlySigR), (REModOnlySigAdjusted$sigma2) - (REModOnlySigR$sigma2)), 3)`.

```{r}
options(scipen = 1, digits = 2)
```

#### Accounting for null results

In assessing the degree to which effects are attenuated between original and replication studies it is important to ask how much this effect is driven by the presence of a subset of replication studies where the null hypothesis is true. The average effect size difference between original studies and their replications could be extremely high, and yet this effect could be entirely driven by cases where the null hypothesis is true. For example, if 50% of studies had true null hypotheses, and yet all non-null replication effects were identical to those reported in the original articles, the average attenuation would be 50% despite the fact that the non-null original effect sizes provided unbiased estimates of the replication effect sizes. Analyses 2 to 4 were performed in order to account for this issue.

##### Analysis 2 and 3: Multilevel random effects meta-analysis with exclusions

Analyses 2 and 3 reperform the above meta-analysis excluding studies using two exclusion criteria. Analysis 2 excluded studies in which the replication study was not statistically significant with an effect in the same direction as the original (using the p value reported in the replication projects’ datasets, at an alpha of .05, and using two-tailed tests where applicable). Analysis 3 removed effects in which the replication study effect was “statistically equivalent” to the null according to an equivalence test.

Analysis 2, excluding studies in which the replication study was not significant, means that replication studies with a low level of statistical power to detect the (unknown) true replication effect size are likely to be excluded. This may lead to this analysis underestimating the amount of effect size exaggeration, as replications with non-zero but small effect sizes are likely to be non-significant. This issue is compounded by the fact that some of the replication projects chose the sample sizes that were used in the replication studies using a power analysis of the observed effect in the original study {Open Science Collaboration, 2015 #611}, meaning that if effect sizes are likely to be smaller in the replication studies replication studies will often be underpowered.

In order to avoid excluding under-powered studies erroneously, Analysis 3 excluded studies based on whether the replication study results were statistically equivalent to the null hypothesis or statistically significant in the opposite direction {Lakens, 2017 #214;Lakens, 2018 #951}.A requirement for equivalence testing is that an equivalence bound is selected (i.e., an effect size below which the effect size is said to be for all practical purposes equal to zero). For this, we used the lowest effect size that would have been statistically significant in the original study (assuming an alpha of .05), following a suggestion in {Lakens, 2018 #951}. Equivalence tests were performed using Z tests of the Fisher z-transformed effect sizes, excluding studies where the observed replication effect was significantly smaller than the equivalence bound using a one tailed test at the 95% confidence level. Standard errors of each study were estimated as $\sqrt{\frac{1}{N-3}}$, except for studies from {Camerer, 2018 #967} that had more than a single replication attempt. In these cases we used the standard errors derived from the meta-analyses that produced the effect size estimate (see Supplementary Materials 1 for details).

In interpreting results based on this exclusion criterion, it is important to note that the minimum detectable effect was occasionally quite high because original sample sizes were often very small (mean equivalence bound in correlation coefficient terms = `r ztor(mean(minimumEffectDetectableZ,na.rm=T))`, SD =  `r sd(ztor(minimumEffectDetectableZ),na.rm=T)`, 0th, 25th, 50th, 75th and 100th quintiles = [`r quantile(ztor(minimumEffectDetectableZ),na.rm=T)`]). This means that original studies were sometimes under-powered to detect even large effects using the current analysis, and as such this method may exclude studies that have replication effects that the original authors may have considered important {Thompson, 2002 #1039}. See supplementary materials 2 for scatter plots of the dataset using each exclusion rule.

##### Analysis 4: Bayesian mixture model

Analyses 2 and 3 both rely on excluding studies using exclusion rules that will, respectively, exclude or retain studies due to low statistical power in the replication study. In part in order to avoid this issue the final approach to estimating the amount of effect size attenuation conditional on the effect under study being non-zero was the Bayesian mixture model presented in {Camerer, 2018 #967}. This model assumes that each replication effect size comes from one of two components, either from the null-hypothesis or from the alternative-hypothesis component. If the true replication effect size is drawn from the null-hypothesis component, it is assumed to be drawn from a normal distribution with a mean of 0. If the true replication effect size comes from the alternative-hypothesis component, then the it is assumed to be drawn from a normal distribution with a mean equal to the original's estimated true effect size attenuated by an "attenuation factor". The attenuation factor is constrained to a value between zero and one and is assumed to be consistent across studies. The observed replication effect size is then assumed to be drawn from a normal distribution centered on the true replication effect size with a standard deviation equal to the standard error of the replication study (estimated here as $\sqrt{\frac{1}{N_2-3}}$, with $N_2$ being the replication sample size). 

There are two main parameters of interest in this model: (i) the "attenuation factor" (called a "deflation factor" in {Camerer, 2018 #967}), which is the degree to which effect sizes are attenuated between original and replication studies, and (ii) the "assignment rate", which is the overall rate at which studies are assigned to the null hypothesis. This analysis was performed in JAGS version 4.3.0 {Depaoli, 2016 #1010} using the rjags interface (version 4.8.0; {Plummer, 2018 #1011}). See Supplementary Materials 4 for model syntax and further analysis details.

### Descriptives

Looking at the `r tableReductions["Overall", "n included"]` included original-replication study pairs included in this analysis, the effect size in the replication study was lower than that in the original study for `r sum(allData$fisherZDiff < 0, na.rm = T)` articles, `r round(mean(allData$fisherZDiff < 0, na.rm = T)*100)`% of the included studies. An exact binomial test shows that this is extremely unlikely under the assumption that replication effect sizes are equally likely to be smaller or larger in the replication study, p `r if(binom.test(sum(allData$fisherZDiff < 0, na.rm = T), tableReductions["Overall", "n included"])$p.value < .001) "< .001"`. The average effect size for original studies was r = `r tableReductions["Overall", "Mean original ES"]`, and the mean effect size for replication studies was r = `r tableReductions["Overall", "Mean replication ES"]`, a mean decrease of r = `r tableReductions["Overall", "Mean original ES"] - tableReductions["Overall", "Mean replication ES"]`. Notably, this represents an average decrease in effect sizes from the original to the replication study of `r abs(round( tableReductions["Overall", "Mean proportion change"]*100))`%. See Table 2 for a comprehensive list of descriptive statistics on the effect size differences seen in this sample and  Figure 1 for a raincloud plot of the Fisher z-score change in effect sizes by replication project.


```{r out.width = 12, out.height = 20}
knitr::include_graphics("Figures/ViolinPlotZscoreChange.jpg")
```

Figure 1. A raincloud plot (density, box and scatter plot) of the change in effect sizes (here Fisher Z scores) from the original to the replication study by the replication project that each replication study was performed as a part of. 


Table 2. 
Differences between original and replication studies. All calculations were performed on Fisher z-transformed correlations and presented effect sizes are back-transformed into correlation coefficients for interpretability.

```{r, echo = F}
kable( t(tableReductions), digits = 2, col.names = c("All studies", "statistically significant replications", "Nonequivalent studies"))
meanDecrease <- tableReductions$`Mean ES difference`[1]
meanPropDecrease <- tableReductions$`Mean proportion change`[1]
rangeDiffNotOverall <- range(tableReductions$`Mean ES difference`[2:nrow(tableReductions)])
rangePropDiffNotOverall <- range(tableReductions$`Mean proportion change`[2:nrow(tableReductions)])
```


### Results
#### Analysis 1: Multilevel random effects meta-analysis results

The random effects meta-analysis including all data estimated a r = `r REModSum$estimate_Cor` (95% CI [`r REModSum$CI95_Cor`]) decrease in effect sizes from the original to replication studies. This represents a decrease equivalent to `r abs(round( (REModSum$estimate_Z/mean(allData$fis.o, na.rm = T))*100))`% (95% CI [`r abs(round( ((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100)[1]))`%, `r abs(round( ((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100)[2]))`%]) of the mean effect size in the original studies (a Fisher z-transformed correlation coefficient equivalent to a correlation  of `r mean(allData$correlation.o, na.rm = T)`). 

```{r}
options(scipen = 1, digits = 3)
```

Greater variance was attributable to the article and effect level than to the project ($\sigma_{article}$ = `r round(sqrt(REMod$sigma2[2]), 3)`, $\sigma_{effect}$ = `r round(sqrt(REMod$sigma2[3]), 3)`, compared to $\sigma_{project}$ = `r round(sqrt(REMod$sigma2[1]), 3)`), representing an intraclass correlation (ICC) for the project of `r round(REMod$sigma2[1] / sum(REMod$sigma2), 3)`. There was a large amount of unexplained heterogeneity, `r niceREModSum[5, "Random effects"]`, $I^2$ = `r Io2` (calculated following {Nakagawa, 2012 #1023}), suggesting that `r round(Io2)`% of variance in effect sizes was due to heterogeneity (i.e., variance in the true effect size differences), while the remaining `r 100-round(Io2)`%  was attributable to sampling variance.

Table 3. Model output from a multilevel random effects meta-analysis of the difference between original and replication effect sizes, with random effects for the project (i.e., which large scale replication project the replication was a part of) and the original (i.e., replicated) article or effect. 

```{r table model summary}
kable(niceREModSum)
```

\n 

```{r blups}
# If you are interested, please uncomment the following for a table of the Empirical Bayes estimates and 95% credible intervals for random effects (i.e., estimates of the difference between the replication project's mean effect size difference and the overall estimated mean effect size difference). These values are equivalent to 95% confidence intervals assuming that the studies are a random sample from a population with normally distributed average effect size difference.

 blups <- BLUPsSource[[1]]
 # row.names(blups) <- str_remove_all(row.names(blups), "\\n")
 blups$Project <- row.names(blups)
 blups <- dplyr::select(blups, Project, everything())
 row.names(blups) <- NULL
 kable(blups, col.names = c("Project" , "Estimate", "Standard Error", "95% PI lower bound", "95% PI upper bound"))

ggplot(blups, aes(x = intrcpt, xmin = pi.lb, xmax = pi.ub , y = Project)) +  geom_errorbarh() +
  geom_point() + theme_classic() + xlab("Empirical Bayes estimates and credible intervals of random effects (Fisher Z score)") + ylab(NULL)

options(scipen = 1, digits = 3)

```

*Figure 2.* Empirical Bayes estimates and 95% credible intervals for the random effect of each replication  project in Fisher Z scores (i.e., estimates of the difference between the replication project's mean effect size difference and the overall estimated mean effect size difference).


#### Analyses 2 and 3: Results from multilevel random effects meta-analysis with exclusions 

Examining just the `r tableReductions["StatisticalSignificance", "n included"]` cases in which the replication study was statistically significant (`r round(tableReductions["StatisticalSignificance", "n included"]/tableReductions["Overall", "n included"],2) * 100`% of all studies), the average effect for the original studies was `r tableReductions["StatisticalSignificance", "Mean original ES"]`, and the mean effect size for replication studies was `r tableReductions["StatisticalSignificance", "Mean replication ES"]`. This represents a mean decrease of r = `r tableReductions["StatisticalSignificance", "Mean original ES"] - tableReductions["StatisticalSignificance", "Mean replication ES"]`, a mean percentage increase in effect sizes of `r abs(round( tableReductions["StatisticalSignificance", "Mean proportion change"]*100))`% and a median percentage decrease of `r abs(round(tableReductions["StatisticalSignificance", "Median proportion change"]*100))`%. Using equivalence testing `r round(( tableReductions["Nonequivalence", "n included"]/ nrow(allData))*100)`% of replication studies were not statistically equivalent to the null (n= `r tableReductions["Nonequivalence", "n included"]`). The average effect size in the original non-equivalent studies was `r tableReductions["Nonequivalence", "Mean original ES"]`, compared to a mean effect size for replication studies of r = `r tableReductions["Nonequivalence", "Mean replication ES"]`. This is a mean decrease of r = `r tableReductions["Nonequivalence", "Mean original ES"] - tableReductions["Nonequivalence", "Mean replication ES"]`, a mean percentage decrease of  `r abs(round(tableReductions["Nonequivalence", "Mean proportion change"]*100))`%,  and a median percentage decrease of `r abs(round(tableReductions["Nonequivalence", "Median proportion change"]*100))`%.

Reperforming the meta-analysis only including studies for which the replication was  statistically significant and had an effect in same direction as the original produced an estimated r = `r modSumariesR$modelEstimate[2]` (95% CI [`r modSumariesR$MLM95lb[2]`, `r modSumariesR$MLM95ub[2]`]) change in effect sizes from original to replication studies. Including only the studies which were not statistically equivalent leads to a predicted r = `r modSumariesR$modelEstimate[3]` (95% CI [`r modSumariesR$MLM95lb[3]`, `r modSumariesR$MLM95ub[3]`]) decrease in effect sizes. The estimates of the proportion of variance attributable to the article or replication project level did not change considerably in either of these subsets. See table [all model output] for the model estimates from each model.

These values represent changes equivalent to a decrease of `r abs(round( ( modRes1$modelEstimate / mean(allData$fis.o, na.rm = T))*100))`% to `r abs(round(( modRes2$modelEstimate / mean(allData$fis.o, na.rm = T))*100))`% of the average original effect size (a correlation coefficient of r = `r ztor(mean(allData$fis.o, na.rm = T))`). However, there was considerable imprecision in these estimates, with 95% confidence intervals for both of these subsamples extending from a considerable decrease equivalent to `r abs(round(( modRes2$MLM95lb  / mean(allData$fis.o, na.rm = T))*100))`% of the average original effect size, to a small increase equivalent to `r round(( modRes1$MLM95ub  / mean(allData$fis.o, na.rm = T))*100)`% of the average original effect size.

##### Table [all model output]
The number of studies included in each model, and the estimated correlation coefficient decrease from each model. 
```{r echo=F}

options(scipen = 1, digits = 2)

colNames <- row.names(modSumariesR) %>% 
        str_replace("Below", " < ") %>%
        str_replace("Above", " > ") %>% 
        str_replace("Significance", "ly significant replications") %>%
        str_replace("Nonequivalence", "Nonequivalent studies") %>%
        str_replace("Overall", "All studies")

rowNames <- c( str_replace(str_replace(names(modSumariesR), "model", "Model "), "MLM95", "95% CI "))

kable(data.frame(rowNames, t(modSumariesR)), col.names = c("Parameter", colNames), row.names = F)

```
Note: Models were estimated using Fisher Z transformed correlation coefficients and back transformed for interpretability. Percentage attenuation gives the percentage attenuation for effect size differences as a percentage of the mean original effect size (r = `r mean(allData$correlation.o, na.rm = T)`). 

##### Leave one out cross validation of meta-analyses

To assess how sensitive the results of the multilevel models were to the inclusion of each of the replication projects, the included studies, and the individual replicated effects, all of the above multilevel models were rerun using leave one out cross validation, excluding both each effect, effects from each original study (i.e., in cases where multiple effects were tested from the same original source), and each replication project one at a time. None of these analyses led to model estimates (i.e., the expected decrease in effect size between original and replication study or equivalently the intercept estimate) that were further than `r LooMaxDiff` from those given above, suggesting that none of the individual projects, effects or studies included were overly influential. See Supplementary Material 3 for tables summarising the leave-one-out model output.

#### Analysis 4: Bayesian mixture model results

The Bayesian mixture model was estimated using four Markov chains from each of which 100,000 draws were taken (excluding an 11,000 draw burn-in period). Trace and density plots for the discussed parameters were examined and, along with and $\hat{R}$ values within .001 of 1, appeared to suggest that the model successfully converged {Gelman, 2011 #1052}. The overall posterior assignment rate (i.e., the proportion of studies that were estimated to be from the non-null alternative hypothesis) was `r round(phiSimple,2)*100`%, with a 95% highest probability density interval of [`r round(phiSimpleHDI, 2 )[1] * 100`%, `r round(phiSimpleHDI, 2 )[2] * 100`%]. The overall attenuation factor (i.e., the estimated amount that effect sizes decreases between the original and replication studies) was `r round((1-as.numeric(alphaSimple))*100)`% with a 95% highest probability density interval of [`r  (1-round(as.numeric(HDISimple)[2], 2))*100`%, `r  (1-round(as.numeric(HDISimple)[1], 2))*100`%]. Figure 2 shows the original effect sizes plotted against replication effect sizes weighted by sample size, along with the posterior assignment rate. The color of each point indicates how often each effect was assigned to the alternative hypothesis.

As was pointed out in {Camerer, 2018 #967}, values close to the diagonal (i.e., cases in which the original and replication effect sizes are similar) were reliably assigned to the alternative hypothesis component whereas effects far below the diagonal wer more often assigned to the null hypothesis componenet. The overall posterior assignment rate might be overly optimistic (i.e., assign studies to the non-null hypothesis at a high rate), likely in part due to the fact that this model allows for "true" effect sizes to be estimated as being extremely low or near zero and still assigned to the alternative hypothesis, with `r round(propBelow.1.BMM, 2) * 100`% of the estimated "true" replication effect sizes being smaller than a correlation coefficient of .10.

```{r,  dpi=300}
mixtureModelPlot + coord_equal(ratio=1)
```

###### Figure 3. 
A scatterplot of replication study effect sizes (in correlation coefficients) plotted against original study effect sizes, colored by the posterior assignment rate, the proportion of times each study was assigned to the alternative hypothesis. Points that fall on the solid, diagonal line represent replication effect sizes equal to the original effect sizes. Point size represents (the log) of the number of participants in the replication study.

## Discussion 

The results show that there was a substantial average decrease in effects sizes between original and replication studies and suggest that this is still the case even after accounting for the presence of null effects. The results of the multilevel meta-analysis show an estimated mean decrease of r = `r REModSum$estimate_Cor`, (95% CI [`r REModSum$CI95_Cor`]), equivalent to a `r REModSum$estimate_D` point Cohen's d decrease (95% CI [`r REModSum$CI95_d`]), or an estimated decrease of `r abs(round((REModSum$estimate_Z/mean(allData$fis.o, na.rm = T)),2))*100`%  (95% CI [`r abs(round((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100))[1]`%, `r abs(round((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100)[2])`%]) of the mean effect size in the original studies (a Fisher z equivalent to r = `r ztor(mean(allData$fis.o, na.rm = T))`). 

Arguably of more interest to researchers examining and planning research is the question of how much effect size attenuation should be expected under the assumption that the effect size is non-zero. All of the methods used here largely agreed, although the degree of precision in their estimates differs. The Bayesian mixture model suggests that there is an average decrease of `r round((1-as.numeric(alphaSimple))*100)`%, with a 95% highest probability density interval of [`r  (1-round(as.numeric(HDISimple)[2], 2))*100`%, `r  (1-round(as.numeric(HDISimple)[1], 2))*100`%]. The multilevel meta-analysis models that excluded non-significant results and studies in which the replications studies were statistically equivalent to the null both led to similar conclusions, although they give less precise estimates, highlighting the uncertainty in these estimates. For example, the confidence intervals over both of the models in Analyses 2 and 3 extend from a meaningful decrease of r = `r ztor(min(modSumariesR$MLM95lb[-1]))` to a trivial  increase of r = `r ztor(max(modSumariesR$MLM95ub[-1]))`.

In using these results to inform future research (e.g., in sample size planning) and to interpret the published literature, it is essential to take note of the level of heterogeneity in the effect size attenuation across not just replicated effects but also across replication projects. The sampling decisions and replication methods used by each of the included projects appears to have had a considerable effect on the amount of effect size attenuation seen (e.g., looking at the results of Analysis 1, the estimated standard deviation of the  mean level of effect size attenuation  across projects is `r round(sqrt(REMod$sigma2[2]), 3)`, 95% CI [`r confIntsREMod[[1]]$random[2,c(2,3)]`]). The degree of heterogeneity in the amount of effect size attenuation across studies and projects means that using any single estimate of the amount of effect size decrease is likely to be misleading in the case of any individual replication study.

```{r, dpi=300}
catPlot 
```

Figure 4. A caterpillar plot of the effect size difference between original and replication study effect sizes ordered by magnitude, error bars are 95% confidence intervals around effect size differences.

#### Limitations and future directions

In interpreting these results it is important to note several limitations. Firstly, the current study does not attempt to distinguish between effect size *heterogeneity* (i.e., effect sizes that are different due to subtle unobserved moderators {Kenny, 2019 #1041} and effect size *exaggeration*. However, in so far as effect size heterogeneity tends to lead to smaller effects in replication studies, it seems reasonable to term this effect size exaggeration for the purposes of researchers hoping to replicate or plan future similar studies of the same type of effects. It also cannot be ruled out that the effect size differences seen in these large-scale replication projects are larger than would be seen by individuals attempting to replicate particular effects (e.g., if researchers in these large-scale replications have less access to the tacit knowledge that would normally facilitate replicators' efforts).

The Bayesian mixture model assumes independence between effects, a uniform attenuation factor across all areas of psychological research, and allows for effects sampled from the alternative distribution to be negligibly small or even negative. Future research could help develop a more nuanced account of the data-generation process underlying this dataset by, for example, building a model that allows for the attenuation rate to change across replication studies, or by including more components in order to allowing for studies with negligible or negative but non-null effects in addition to the alternative and null components.

The large amount of heterogeneity across the replication projects reinforces the need for future replication projects to attempt to closely replicate a random sample from defined populations of studies. While the current set of replication projects allows us to make reasonable inferences about future replication projects, the varied sampling strategies employed by the previous projects necessarily limit the predictions and inferences we can draw. At the moment, it is difficult to say whether the heterogeneity in effect size attenuation are due to intra-field differences in replicability and effect size attenuation, differences caused by the sampling strategies, or other issues such as differences in the quality of the replication studies.

In order to be able to start to disentangle these different possible causes and develop an account of the predictors and moderators of replication success and effect size attenuation, we need to begin to develop a large representative database of replication studies. Especially if such a database could be augmented with meta-data regarding the type of analysis, design and effects under study, this resource could allow us to make meaningful predictions about individual future replications. However, until a large database of such studies becomes available, analyses like the current one provide the best estimates possible, albeit estimates that should be read and understood with these caveats in mind.


### Conclusion 

The findings of this study reinforce the importance of recent efforts to reduce psychology literature’s reliance on underpowered original research designs, to circumvent publication bias, and to avoid QRPs like p-hacking and HARKing {Bakker, 2012 #38}. Efforts to avoid the impact of any of these issues would likely reduce the degree to which effect sizes are attenuated in replications of the primary research literature. 

In order to avoid performing future underpowered research, researchers should be aware that their experiments are likely to be underpowered if they plan their sample sizes using the effect size reported in a previous experiment. As a conservative heuristic for researchers performing formal sample size planning on the basis of previous research, researchers could follow the advice given in {Camerer, 2018 #967} aand plan their experiments assuming that the true effect size is 50% of the reported effect size, a value matched by the more extreme 95% confidence interval of the estimated amount of effect size exaggeration across studies in this sample. Alternatively, it may be preferable to use methods of sample size planning that do not rely on precise a priori estimation of the effect size under study, such as planning studies to reliably detect the smallest effect size of interest {Lakens, 2018 #951}, using sequential analysis strategies {Pocock, 1977 #553}, or planning for adequate precision in parameter estimates across a range of possible effect sizes {Maxwell, 2008 #559;Kelley, 2017 #727}. In addition, recent large-scale multinational data collection efforts like the Many labs Projects or the Psychological Science Accelerator {Moshontz, 2018 #1025} also help to avoid the negative impacts of low statistical power by allowing for extremely high powered studies of even very small effects.

There are several recent efforts to reduce the impact of publication and reporting biases that readers should be aware of, many of which individual researchers can voluntarily and easily take part in. Careful preregistration of analysis plans allows researchers to avoid biases in the analysis of their data that may otherwise lead to inflated effect sizes{Wicherts, 2016 #475}. Data-sharing platforms such as figshare (figshare.com) and the Open Science Framework (osf.io) make it possible for researchers to easily share the results of research whether or not a study is published in a traditional journal. Similarly, pre-prints (e.g., https://psyarxiv.com) allow researchers to report and publicize reports and data that may otherwise remain in the file draw. Both preprints and data repositories make it easier to ensure that non-significant results are accessible to other researchers and meta-analysts. Finally, registered reports, in which papers are reviewed before data-collection on the basis of the research design and analysis strategy as opposed to the results, also show promise in helping to develop a body of literature that is not affected by reporting and publication bias {Nosek, 2014 #202}. However, until large bodies of research free of publication bias become available, researchers should be aware that effect sizes in published studies are likely to be considerably overstated.


## Supplementary material 
## SM1 
###  Replication project Extraction and exclusion details

##### {Open Science Collaboration, 2015 #611}
Three original studies which did not report that their findings were indicative of a non-zero effect were excluded from those studies extracted from {Open Science Collaboration, 2015 #611}. Three studies for which z transformed correlation coefficients could not be extracted due to missing data in the downloaded data set were also excluded from analysis (these included 1 study which used multiple statistical tests in the original and replication studies, one study in which the replication and original study used different statistical tests, and one study for which the effect sizes were reported as beta coefficients wihtout test statistics or degrees of freedom). Effect sizes for original and replication studies are included for `r nrow(data)` out of 97 studies replicated studies from {Open Science Collaboration, 2015 #611} which reported having found a non-zero effect.

##### {Camerer, 2018 #967}
Original and replication effect sizes were extracted for all `r nrow(data4)` studies included in {Camerer, 2018 #967}. In some cases in the Nature Science reproducibility project {Camerer, 2018 #967} multiple replication studies were performed for a single effect. In each of these cases we performed a fixed effects meta-analysis using the metafor package {Viechtbauer, 2010 #796} to estimate a meta-analytic effect size estimate. The effect size, standard errors and sample sizes used in the current study reflect this pooled estimate. This method leads to one study more "replicating" according to the 'statistical significance in the same direction of the original study' criterion than was originally reported in {Camerer, 2018 #967}, where they used the p value from the largest performed study instead of a pooled estimate.

##### {Soto, in press #1032} 
Effect sizes were extracted for original and replication studies for `r sum(!is.na(data7$correlation.o) & !is.na(data7$correlation.r))` out of `r nrow(loopr)` included studies, and one original study's sample size was not available. In {Soto, in press #1032} effect sizes which were only reported in this dataset as beta coefficients were not converted to Fisher z scores as not enough information was available in the data set to do so. A total of `r sum(allData$abrev == "loopr")` of 121 effects were included in the current analysis. As some replication studies used shorter form versions of the original data collection instruments, all results presented have been disattenuated using the Spearman-Brown prediction formula and Spearman disattenuation formula to estimate the trait-outcome associations that would be expected if the outcome measure had used the same number of items as the original study (Lord & Novick, 1968). Following the other large scale replication studies, the signs of the original and replication study effects were inverted for analysis if the original effect was negative. 

##### {Cova, 2018 #984}
{Cova, 2018 #984} included three replications of original studies which were non-significant (and which did not claim to provide evidence for the effects under test), these were removed from analysis. Effect sizes were reported by Cova et al. (2018) and are included in the current study for `r sum(!is.na(data6$fis.o) & !is.na(data6$fis.r) & !is.na(data6$n.o) & !is.na(data6$n.r))`, original and replication studies, out of an original 37 replicated studies with significant original results. The four studies for which no effect sizes were reported performed analyses for which Cova et al. (2018) could not develop reasonable effect size estimates (e.g., a Sobel test, GEE analysis).

##### Many labs 1 {Klein, 2014 #988}
Many labs 1 {Klein, 2014 #988} examined whether effects from 13 original papers replicated, one of which did not report an effect size or  test statistic so is not included in the current sample. No effect size was extractable for one original study, and this effect was excluded for the purposes of the current analysis. Four different operationalisations of anchoring effects were tested, all of which are included in the current analysis, leading to a total of 15 paired data-points being included from this study. The multilevel models reported below accounts for non-independence between effects by including a random effect for study. 

##### Many labs 2 {Klein, 2018 #1021}
A total of `r nrow(data8)` of `r nrow(data8) + 6 ` paired original and replication effects sizes are included for this analysis. Four studies from {Klein, 2018 #1021}  were removed because the original and replication studies examined a difference in effect sizes seen in different conditions, and the effects were not directly tested against each other making it difficult to derive an appropriate effect size. Two additional studies were excluded because their effect sizes were only available as Cohen's q.

##### {Ebersole, 2016 #985}
Original and replication effect sizes were extracted for all 9 original and replication studies from {Ebersole, 2016 #985}, excluding a study they term a "conceptual replication". Most effects (6/9) were converted to correlation coefficients from the Cohen's d values reported in this replication project. The results of three additional studies reported as partial Eta squared were converted to correlation coefficients from F statistics using the formula: 

$r\  =\ \sqrt{\frac{F_{obs}\times(df_1 / df_2)}{F_{obs} \times (df_1 / df_2) + 1}} \times \sqrt{\frac{1}{df_1}}$

##### {Camerer, 2016 #983}
The economics replication project {Camerer, 2016 #983}. Original and replication effect sizes for all `r sum(allData$abrev == 'econ')` studies were reported in correlation coefficients and all are included in this analysis.

### SM2 
### Plots and multilevel model output of the relationship between original and replication correlation coefficients using varied exclusion criteria

The following output shows scatter plots and model output for all of the multilevel meta-analyses performed using the varied exclusion criteria explained in the main text.

Table SM `r tabSMN <- 1; tabSMN`. Multilevel meta-analysis model estimates and random effects for all data.

```{r}
kable(niceModelSums$`All Data`)
```

```{r}
plotAllData 
```

Figure SM`r figSMN <- 1; figSMN`. Scatter plot of replication effect sizes (in correlation coefficients) plotted against original effects including all data.


Table SM`r tabSMN <- tabSMN + 1; tabSMN`. Multilevel meta-analysis model estimates and random effects including only statistically significant replications. 

```{r}
kable(niceModelSums$`Only Signficant replications`)
```


```{r}
plotSigR 
```

Figure SM`r figSMN <- figSMN+1; figSMN`. Scatter plot of replication effect sizes (in correlation coefficients) plotted against original effects including only statistically significant replications. 


Table SM`r tabSMN <- tabSMN + 1; tabSMN`. Multilevel meta-analysis model estimates and random effects including studies which are not statistically equivalent to the null, using equivalence bounds set as the minimum effect size that would have been statistically significant in the original study. 

```{r}
kable(niceModelSums$`Non-equivalent studies`)
```

```{r}
plotNonequiv 
```

Figure SM`r figSMN <- figSMN+1; figSMN`. Scatter plot of replication effect sizes (in correlation coefficients) plotted against original effects including studies which are not statistically equivalent to the null, using equivalence bounds set as the minimum effect size that would have been statistically significant in the original study.


### SM3
#### LOO Cross validation output

#### Table [LOO cross validation output].
0th, 25th, 50th, 75th and 100th percentiles from leave one out cross validation for each multilevel model, for each exclusion method an, including only the sample indicated in "LOO exclusions". 

```{r}
kable(LOOoutput)
```

### SM4
#### Bayesian Mixture Model

The mixture model results presented in text presents the model developed by Camerer et al,. (2018; see https://osf.io/xhj4d/ for their detailed description of this model).  All priors were chosen to be uninformative or vague. The mixture model assumes that the observed replication effect sizes either come from the null hypothesis, a true effect sampled from a normal distribution with a mean of zero and a estimated precision (tau). This model uses an errors-in-variables approach to account for possible attenuation of effect sizes due to measurement error and estimation uncertainty following {Matzke, 2017 #1012}, which means the effect size attenuation factor is the factor change between the estimated true effect of the original and replication study effect size. Although this may be reasonable in that the true effect size of the effect may not be the true effect size of a particular study and analysis set up, this poses an interpretative problem in that alpha now represents the difference between the estimated original effect and the replication effect.

Box SM1. The original model reported in {Camerer, 2018 #967} and reported on in the main text of the current article. 

```{}
model{
# Mixture Model Priors:
alpha ~ dunif(0,1) # flat prior on slope for predicted effect size under H1
tau ~ dgamma(0.001,0.001) # vague prior on study precision
phi ~ dbeta(1, 1) # flat prior on the true effect rate
# prior on true effect size of original studies:
for (i in 1:n){
trueOrgEffect[i] ~ dnorm(0, 1)
}
# Mixture Model Likelihood:
for(i in 1:n){
clust[i] ~ dbern(phi)
# extract errors in variables (FT stands for Fisher-transformed):
orgEffect_FT[i] ~ dnorm(trueOrgEffect[i], orgTau[i])
repEffect_FT[i] ~ dnorm(trueRepEffect[i], repTau[i])
trueRepEffect[i] ~ dnorm(mu[i], tau)
# if clust[i] = 0 then H0 is true; if clust[i] = 1 then H1 is true and
# the replication effect is a function of the original effect:
mu[i] <- alpha * trueOrgEffect[i] * equals(clust[i], 1)
# when clust[i] = 0, then mu[i] = 0;
# when clust[i] = 1, then mu[i] = alpha * trueOrgEffect[i]
  }
}
```


### SM5

#### Conversions

All statistical tests extracted were transformed into correlation coefficients as follows, using the methods reported in {Open Science Collaboration, 2015 #611}. 

t statistics:

$r\ =\ \sqrt{\frac{t_{obs}^2\times(1 / df)}{(t_{obs}^2 / df) + 1}}$

Where $t_{obs}$ is the observed t statistic and $df$ is the degrees of freedom of the t test. 

F statistics:

$r\  =\ \sqrt{\frac{F_{obs}\times(df_1 / df_2)}{F_{obs} \times (df_1 / df_2) + 1}} \times \sqrt{\frac{1}{df_1}}$

Where $F_{obs}$ is the observed F statistic and $df_1$ is the degrees of freedom of the numerator and $df_2$ is degrees of freedom of the denominator.  

Chi square statistics:

$r\ =\ \sqrt{\frac{\chi^2_{obs}}{df + 2}}$

Where $\chi^2_{obs}$ is the observed $\chi^2_{obs}$ statistic and $df$ is the associated degrees of freedom.

All values were then transformed into fisher Z transformed correlation coefficients using:

$z\ = \frac 12 \times \ln{\Big(\frac{1 + r}{1 - r}}\Big)$
  
