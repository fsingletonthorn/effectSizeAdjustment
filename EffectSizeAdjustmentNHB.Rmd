---
title: "EstimatingPublicationBias"
output:
  word_document
---

```{r message=FALSE, warning=FALSE, include=FALSE}
source(file = 'Data/data_collection_cleaning.R')
source(file = 'simplifiedAnalysisScript.R')
options(scipen = 1, digits = 2)
knitr::opts_chunk$set(fig.width=8, fig.height=5, echo =FALSE) 
options(knitr.kable.NA = '')
```

## Introduction
See word document 


### Results

Original and replication effect sizes were extracted for a total of `r tableReductions["Overall", "n included"]` studies performed as part of the eight published or in press large-scale replication projects in the behavioral science research literature and converted to Fisher Z values for comparability. See the methods section below for full details regarding exclusion criteria and effect size conversions. The effect size seen in the replication study was lower than that seen in the original study in `r sum(allData$fisherZDiff < 0, na.rm = T)` articles, `r round(mean(allData$fisherZDiff < 0, na.rm = T)*100)`% of the included studies. An exact binomial test shows that this is extremely unlikely under the assumption that replication effect sizes are equally likely to be smaller or larger in the replication study, p = `r if(binom.test(sum(allData$fisherZDiff < 0, na.rm = T), tableReductions["Overall", "n included"])$p.value < .001) "< .001"`. The mean effect size for original studies was r = `r tableReductions["Overall", "Mean original ES"]`, and the mean effect size for replication studies was r = `r tableReductions["Overall", "Mean replication ES"]`, a mean decrease of r = `r tableReductions["Overall", "Mean original ES"] - tableReductions["Overall", "Mean replication ES"]`. Notably, this represents an average decrease in effect sizes from the original to the replication study of `r abs(round( tableReductions["Overall", "Mean proportion change"]*100))`%. See Table 2 for descriptives on the effect size differences seen in this sample and Figure 1 for a raincloud plot of the Fisher Z score change in effect sizes by replication project.


```{r}
knitr::include_graphics("Figures/ViolinPlotZscoreChange.jpg")
```

Figure 2. A raincloud plot of the change in effect sizes (here Fisher Z scores) from the original to the replication study by the replication project that each replication study was performed as a part of. 

Table 2. 
Differences between original and replication studies. All calculations were performed on Fisher's Z transformed correlations and presented effect sizes are back-transformed into correlation coefficients for interpretability.

```{r, echo = F}
kable( t(tableReductions), digits = 2, col.names = c("All studies", "statistically significant replications", "Nonequivalent studies"))
meanDecrease <- tableReductions$`Mean ES difference`[1]
meanPropDecrease <- tableReductions$`Mean proportion change`[1]
rangeDiffNotOverall <- range(tableReductions$`Mean ES difference`[2:nrow(tableReductions)])
rangePropDiffNotOverall <- range(tableReductions$`Mean proportion change`[2:nrow(tableReductions)])

options(scipen = 1, digits = 3)
```


#### Analysis 1: Multilevel random effects meta-analysis results


In order to estimate the expected effect size decrease between original and replication studies this study used a random effects meta-analytic framework. This analysis treats each pair of effects, from the original article and the replication, as one "study", and uses inverse variance weighting in order to estimate the expected effect size decrease between original and replication projects. In order to account for non-independence and clustering in the included difference scores, random effects for replication project and original article were included (as in some cases, multiple effects from an original article were replicated or multiple operationalisations of an original effect were tested), and random effects were also included for each individual effect size difference. 

This analysis showed an estimated r = `r REModSum$estimate_Cor` (95% CI [`r REModSum$CI95_Cor`]) decrease in effect sizes from the original to replication studies. This represents a considerable decrease equivalent to `r abs(round( (REModSum$estimate_Z/mean(allData$fis.o, na.rm = T))*100))`% (95% CI [`r abs(round( ((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100)[1]))`%, `r abs(round( ((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100)[2]))`%]) of the mean effect size in the original studies (a Fisher Z transformed correlation coefficient equivalent to a correlation coefficient of `r mean(allData$correlation.o, na.rm = T)`). 

```{r}
options(scipen = 1, digits = 3)
```

More variance was attributable to the article and effect level than to the project ($\sigma^2_{article}$ = `r round(REMod$sigma2[2], 3)`, $\sigma^2_{effect}$ = `r round(REMod$sigma2[3], 3)`, compared to $\sigma^2_{project}$ = `r round(REMod$sigma2[1], 3)`), representing an intraclass correlation (ICC) for the project of `r round(REMod$sigma2[1] / sum(REMod$sigma2), 3)`. There was a large amount of unexplained heterogeneity, `r niceREModSum[5, "Random effects"]`, $I^2$ = `r Io2` ($I^2$ calculated following {Nakagawa, 2012 #1023}), suggesting that `r round(Io2)`% of variance in effect sizes was due to heterogeneity (i.e., variance in the true effect size differences), while the remaining `r 100-round(Io2)`%  was attributable to sampling variance. 

Table [nice mod sum]. Model output from a multilevel random effects meta-analysis of the difference between original and replication effect sizes, with random effects for the project (i.e., which large-scale replication project the replication was a part of) and the original (i.e., replicated) article or effect. 

```{r table model summary}
kable(niceREModSum)
```



### Accounting for null effects 

An important question in assessing the degree to which effects are attenuated in this literature is how much the attenuation is driven by the presence of a subset of null effects. The average effect size difference between original studies and their replications could be could be extremely high, and yet but this effect could be entirely driven by the presence of null effects (e.g., if 50% of studies examined scenarios where there was no between-group difference or association at the population level, and yet all non-zero effects are identical to those reported, the average attenuation would be 50%). We used two main methods to estimate the expected effect size decrease conditional on the true effect size being non-zero, firstly re-performing the above meta-analysis excluding studies using two exclusion criteria (analyses 2 and 3) and secondly using the Bayesian mixture model originally presented in {Camerer, 2018 #967} (analysis 4). 

In analysis 2, we excluded studies in which the replication study was not statistically significant with an effect in the same direction as the original (using an alpha of .05 and a two-tailed test when applicable). Analysis 2, excluding studies in which the replication study was not significant, means that replication studies which had a low level of statistical power to detect the true effect size under study were likely to be excluded. Especially as in some of the replication projects the sample size in the replication study was chosen using a power analysis of the (possibly inflated) observed effect in the original study {Open Science Collaboration, 2015 #611}, this method is likely to underestimate the amount of effect size exaggeration due to the exclusion of under-powered replications.

In order to avoid excluding under-powered studies erroneously, analysis 3 excludes studies based on whether the results are statistically equivalent to the null hypothesis, or statistically significant in the opposite direction {Lakens, 2017 #214;Lakens, 2018 #951}. A requirement for equivalence testing is that an equivalence bound is selected (i.e., an effect size below which the effect size is said to be for all practical purposes equal to zero). For this, we use the lowest effect size that would have been statistically significant in the original study (assuming an alpha of .05), following a suggestion in {Lakens, 2018 #951}. Equivalence tests were performed using Z tests of the Fisher Z transformed effect sizes, excluding studies where the observed replication effect is significantly smaller than the equivalence bound using a one tailed test at the 95% confidence level. 

#### Analysis 2 - 3: Multilevel random effects meta-analysis with exclusions 
Examining just the `r tableReductions["StatisticalSignificance", "n included"]` cases in which the replication study was statistically significant (`r round(tableReductions["StatisticalSignificance", "n included"]/tableReductions["Overall", "n included"],2) * 100`% of all studies), the average effect for original studies was `r tableReductions["StatisticalSignificance", "Mean original ES"]`, and the mean effect size for replication studies was `r tableReductions["StatisticalSignificance", "Mean replication ES"]`. This represents a mean decrease of r = `r tableReductions["StatisticalSignificance", "Mean original ES"] - tableReductions["StatisticalSignificance", "Mean replication ES"]`, a mean percentage increase in effect sizes of `r abs(round( tableReductions["StatisticalSignificance", "Mean proportion change"]*100))`% and a median percentage decrease of `r abs(round(tableReductions["StatisticalSignificance", "Median proportion change"]*100))`%. Using equivalence testing `r round(( tableReductions["Nonequivalence", "n included"]/ nrow(allData))*100)`% of replication studies were not statistically equivalent to the null (n= `r tableReductions["Nonequivalence", "n included"]`). The average effect size in the original non-equivalent studies was `r tableReductions["Nonequivalence", "Mean original ES"]`, compared to a mean effect size for replication studies of r = `r tableReductions["Nonequivalence", "Mean replication ES"]`. This is a mean decrease of r = `r tableReductions["Nonequivalence", "Mean original ES"] - tableReductions["Nonequivalence", "Mean replication ES"]`, a mean percentage decrease of  `r abs(round(tableReductions["Nonequivalence", "Mean proportion change"]*100))`%,  and a median percentage decrease of `r abs(round(tableReductions["Nonequivalence", "Median proportion change"]*100))`%.

Reperforming the meta-analysis only including studies for which the replication was  statistically significant and had an effect in same direction as the original produced an estimated r = `r modSumariesR$modelEstimate[2]` (95% CI [`r modSumariesR$MLM95lb[2]`, `r modSumariesR$MLM95ub[2]`]) change in effect sizes from original to replication studies. Including only the studies which were not statistically equivalent to the null leads to a predicted r = `r modSumariesR$modelEstimate[3]` (95% CI [`r modSumariesR$MLM95lb[3]`, `r modSumariesR$MLM95ub[3]`]) decrease in effect sizes. The estimates of the proportion of variance attributable to the article or replication project level did not change considerably in either of these subsets. See table [all model output] for the model estimates from each model.

These values represent changes equivalent to a decrease of `r abs(round( ( modRes1$modelEstimate / mean(allData$fis.o, na.rm = T))*100))`% to `r abs(round(( modRes2$modelEstimate / mean(allData$fis.o, na.rm = T))*100))`% of the average original effect size (a correlation coefficient of r = `r ztor(mean(allData$fis.o, na.rm = T))`). However, there was considerable imprecision in these estimates, with 95% confidence intervals for both of these subsamples extending from a considerable decrease equivalent to `r abs(round(( modRes2$MLM95lb  / mean(allData$fis.o, na.rm = T))*100))`% of the average original effect size, to a small increase equivalent to `r round(( modRes1$MLM95ub  / mean(allData$fis.o, na.rm = T))*100)`% of the average original effect size.

##### Table [all model output]
The number of studies included in each model, and the estimated correlation coefficient decrease from each model. Percentage attenuation gives effect size differences as a percentage of the mean original effect size across the data set (r = `r mean(allData$correlation.o, na.rm = T)`). 
```{r echo=F}

options(scipen = 1, digits = 2)

colNames <- row.names(modSumariesR) %>% 
        str_replace("Below", " < ") %>%
        str_replace("Above", " > ") %>% 
        str_replace("Significance", "ly significant replications") %>%
        str_replace("Nonequivalence", "Nonequivalent studies") %>%
        str_replace("Overall", "All studies")

rowNames <- c( str_replace(str_replace(names(modSumariesR), "model", "Model "), "MLM95", "95% CI "))

kable(data.frame(rowNames, t(modSumariesR)), col.names = c("Parameter", colNames), row.names = F)

```

##### Leave one out cross validation of meta-analyses

To assess how sensitive the results of the multilevel models were to the inclusion of each of the replication projects, the included studies, and the individual replicated effects, all of the above multilevel models were rerun using leave one out cross validation, excluding each individual effect size decrease, effects from each original study (i.e., in cases where multiple effects were tested from the same original source), and each replication project one at a time. None of these analyses led to model estimates (i.e., the expected decrease in effect size between original and replication study) that were further than `r LooMaxDiff` from those given above, suggesting that none of the individual projects, effects or studies included were overly influential. See supplementary material 3 for tables summarising the leave-one-out model output.

#### Analysis 4: Bayesian mixture model results
Analyses 2 and 3 both rely on excluding studies using exclusion rules that will, respectively, exclude or retain studies due to low statistical power in the replication study. In part in order to avoid this issue the final approach we used to estimate the amount of effect size attenuation that should be expected conditional on the effect under study being non-zero was the Bayesian mixture model presented in {Camerer, 2018 #967}. This model assumes that each observed replication effect size comes from one of two components, either from the null-hypothesis or from the alternative-hypothesis. See the methods section below for more details on how this model was specified and supplementary materials 4 for full model syntax. There are two main parameters of interest in this model; the “attenuation factor” (called a deflation factor in {Camerer, 2018 #967}), the degree to which effect sizes are attenuated between original and replication studies when the estimated "true" effect sizes are non-null, and the overall rate at which studies are assigned to have come from the null hypothesis (the “assignment rate”). 

This model Bayesian mixture model was estimated using four Markov chains from each of which 100,000 draws were taken (excluding an 11,000 draw burn-in period). Trace and density plots for the discussed parameters were assessed and the model appeared to have converged. The overall posterior assignment rate (i.e., the proportion of studies which are estimated to be from the non-null alternative hypothesis) is `r round(phiSimple,2)*100`%, with a 95% highest probability density interval of [`r round(phiSimpleHDI, 2 )[1] * 100`%, `r round(phiSimpleHDI, 2 )[2] * 100`%]. The overall attenuation factor (i.e., the estimated amount that effect sizes decreases between the original and replication studies) is `r round((1-as.numeric(alphaSimple))*100)`% with a 95% highest probability density interval of [`r  (1-round(as.numeric(HDISimple)[2], 2))*100`%, `r  (1-round(as.numeric(HDISimple)[1], 2))*100`%]. Figure [mixture model], shows the original effect sizes plotted against replication effect sizes weighted by sample size, along with the posterior assignment rate. The color of each point indicates how often each effect was assigned to the alternative hypothesis.

As was pointed out in the first use of this model in {Camerer, 2018 #967}, values close to the diagonal (i.e., cases in which the original and replication effect sizes are similar) are reliably assigned to the alternative hypothesis whereas effects far below the diagonal are more often assigned to the null hypothesis. The overall posterior assignment rate might be overly optimistic (i.e., assign studies to the non-null hypothesis at a high rate), likely in part due to the fact that this model allows for "true" effect sizes to be estimated as being extremely low or near zero and still assigned to the alternative hypothesis, with `r round(propBelow.1.BMM, 2) * 100`% of the estimated "true" replication effect sizes being smaller than a correlation coefficient of .1.

```{r}
mixtureModelPlot
```

###### Figure [mixture model]. 
A scatterplot of replication study effect sizes (in correlation coefficients) plotted against original study effect sizes, colored by the posterior assignment rate, the proportion of times each study was assigned to the alternative hypothesis. Points which fall on the solid, diagonal line represent replication effect sizes equal to the original effect sizes. Point size represents (the log) of the number of participants in the replication study.

## Discussion 

These results show that there was a substantial average decrease in effects sizes between the original and replication study and suggest that this is still the case even after accounting for the presence of null effects. The results of the multilevel meta-analysis results show an estimated mean decrease of r = `r REModSum$estimate_Cor`, (95% CI [`r REModSum$CI95_Cor`]), equivalent to a `r REModSum$estimate_D` point Cohen's d decrease (95% CI [`r REModSum$CI95_d`]), or an estimated decrease of `r abs(round((REModSum$estimate_Z/mean(allData$fis.o, na.rm = T)),2))*100`%  (95% CI [`r abs(round((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100))[1]`%, `r abs(round((REModSum$CI95_Z/mean(allData$fis.o, na.rm = T))*100)[2])`%]) of the mean effect size in the original studies (a Fisher Z equivalent to r = `r ztor(mean(allData$fis.o, na.rm = T))`). 

Arguably of more interest to researchers examining and planning research is the question of the degree of effect size attenuation expected under the assumption that the effect size is non-zero. All of the methods used here largely agreed, although the degree of precision in their estimates differs. The Bayesian mixture model suggests that there is an average decrease of `r round((1-as.numeric(alphaSimple))*100)`%, with a 95% highest probability density interval of [`r  (1-round(as.numeric(HDISimple)[2], 2))*100`%, `r  (1-round(as.numeric(HDISimple)[1], 2))*100`%]. The multilevel models estimated excluding non-significant results and studies in which the replications studies were statistically equivalent to the null lead to similar conclusions, although they give less precise estimates, highlighting the uncertainty in these estimates. For example, the confidence intervals over both of the models estimated excluding data extend from a meaningful decrease of r = `r ztor(min(modSumariesR$MLM95lb[-1]))` to a slight increase of r = `r ztor(max(modSumariesR$MLM95ub[-1]))`. 

```{r}
catPlot 
```

Figure x. A caterpillar plot of the effect size difference between original and replication study effect sizes ordered by magnitude, error bars are 95% confidence intervals around effect size differences.

#### Limitations and future directions

In interpreting these results it is important to note several limitations. Firstly, the current study does not attempt to distinguish between effect size heterogeneity (i.e., effect sizes that are different under due to subtle unobserved moderators {Kenny, 2019 #1041} and effect size exaggeration. However, in so far as effect size heterogeneity tends to lead to smaller effects in replication studies, it seems reasonable to term this effect size exaggeration for the purposes of researchers hoping to replicate or plan future similar studies of the same type of effects. It also cannot be ruled out on the basis of this data that the effect size differences seen in these large-scale replication projects are larger than would be seen for individuals attempting to replicate particular effects (e.g., if researchers in these large-scale replications have less access to the tacit knowledge that would normally facilitate replicators efforts).

The Bayesian mixture model presented above assumes independence between effects and a uniform attenuation factor across all areas of psychological research, and allows for modeled true effect sizes to be negligibly small or even negative and still assumed to be sampled from the alternative distribution. Future research could help develop a more nuanced account of the data-generation process underlying this dataset by, for example, building a model that allows for the attenuation rate to change across replication studies, or by including more components in order to allowing for studies with negligible or negative but non-null effects in addition to the true alternative and null components modeled.

Most importantly, none of the replication projects included in this analysis replicated true random selections from the literature, and the sampling strategies of the replication projects included vary widely (e.g., Soto, in press, examines studies included in a previous overview of trait-outcome associations whereas Camerer et al., 2018 only included studies published between 2010 and 2015 in the journals Nature and Science). It is possible that the effect size decreases seen here are systematically different from what would be seen across the behavioural sciences literature. While these results should be considered preliminary, this analysis nonetheless provides an initial estimate of the amount of effect size attenuation that should be expected when planning research, and suggests that even accounting for the presence of null effects the amount of effect size attenuation between the published literature and replication studies is still noteworthy.

### Conclusion 

Researchers should be aware that their experiments are likely to be underpowered if they plan their sample sizes using the effect size reported in a previous experiment. As a conservative heuristic for researchers performing formal sample size planning such as a power analysis on the basis of previous research, researchers could follow the advice given in {Camerer, 2018 #967} and plan their experiments assuming that the true effect size is 50% of the reported effect size, a value matched by the more extreme 95% confidence interval of the estimated amount of effect size exaggeration across studies in this sample. Alternatively, it may be preferable to use methods of sample size planning that do not rely on precise a priori estimation of the effect size under study, such as planning studies to reliably detect the smallest effect size of interest {Lakens, 2018 #951}, using sequential analysis strategies {Pocock, 1977 #553;Lakens, 2014 #169}, or planning for adequate precision in parameter estimates across a range of possible effect sizes {Maxwell, 2008 #559;Kelley, 2017 #727}.

This research also emphasizes the importance of efforts to reduce publication and reporting biases, many of which individual researchers can voluntarily and easily take part in. Firstly, data-sharing platforms such as figshare (figshare.com) and the Open Science Framework (osf.io) make it possible for researchers to easily share the results of research whether or not a study is published in a traditional journal. Secondly, pre-prints (e.g., https://psyarxiv.com) allow researchers to report and publicize reports and data that may otherwise remain in the file draw, making it easier to ensure that non-significant results are accessible to other researchers and meta-analysts. Finally, projects like registered reports, in which papers are reviewed before data-collection on the basis of the design and analysis strategy as opposed to the results, also show promise in helping to develop a body of literature which is not affected by reporting and publication bias {Nosek, 2014 #202}. However, until large bodies of research free of publication bias become available, researchers should be aware that effect sizes in published studies are likely to be considerably overstated.

## Methods 
### Data extraction

All eight published or in press large-scale replication projects performed within in the behavioral science research literature were included in the current research (see Table 1 for a list of the included studies and supplementary materials 1 for extended details about which studies were included or not included). The original source of each replicated effect, reported test statistics, effect sizes, sample sizes, standard errors and p-values were extracted for each original and replication study. Several of the large-scale replication projects did not present the original test statistics and p values (e.g., Many labs 1 and 3 {Klein, 2014 #988;Ebersole, 2016 #985}. In these cases, these values were manually extracted from the original articles. When sample sizes for original studies were not reported in the data provided by each replication project they were manually extracted from original articles where possible.

For analysis, the original and replication effect sizes were transformed to Fisher z Transformed correlation coefficients following the methods used in {Open Science Collaboration, 2015 #611}, see supplementary materials 5 for details regarding these conversions. This conversion used data from the replication project whenever possible (i.e., whenever effect sizes were reported in correlation coefficients in a summary table or in a project's data this was used and directly converted to  Fisher z values). Cases where the study-level results were not reported in correlation coefficients, Cohen's d values, as t-tests, or as F statistics were excluded from analysis (e.g., cases when no effect size was reported in the original study or in the replication project data set). In cases where sample sizes were not reported per group, sample sizes among groups were assumed to be equal in these conversions. See supplementary materials 1 for a comprehensive account of exclusions and study specific extraction details for each replication project. See Table 1 for the number of valid studies extracted from each project. An original and replication effect size that could be converted to a Fisher z score along with sample sizes for original and replication studies was extracted for a total of `r sum(!is.na(allData$fis.o) & !is.na(allData$fis.r) & !is.na(allData$n.o) & !is.na(allData$n.r) )` studies, excluding a total of  `r 347 - sum(!is.na(allData$fis.o) & !is.na(allData$fis.r) & !is.na(allData$n.o) & !is.na(allData$n.r) )` effect size differences.

### Analysis

All analysis was performed in R version 3.5.1 {R Development Core Team, 2018 #314} and meta-analyses were performed using the Metafor package version 2.1 {Viechtbauer, 2010 #796} using restricted maximum-likelihood estimation. All analyses and difference scores (i.e., proportion changes and mean differences) were calculated using Fisher Z transformed effect sizes, and effect sizes are back transformed to correlation coefficients for easy interpretation unless otherwise stated. All analyses were exploratory, and multiple models which were developed are not presented here. See https://github.com/fsingletonthorn/effectSizeAdjustment for a git repository with a record of all interim models and for all model code and data, and see https://osf.io/daj8b for the preregistration of this project specifying that all analyses would be exploratory.

##### Analysis 1: Multilevel random effects meta-analysis 

Analysis 1 uses a random effects meta-analysis framework to estimate the expected effect size decrease between original and replication studies.

$difference_{j} = \gamma_0  + u_{id} + u_{article} + u_{project} + e_{j}$

This analysis treats each pair of effects, the original and replicated effect size, as one "study" in a meta-analytic framework, and estimates the change from original to replication study ($differnce_{j}$). In order to account for non-independence between the included difference scores, random effects for replication project ($u_{project}$) and original article ($u_{article}$) are included, and random effects are also included for each individual effect ($u_{id}$). Standard errors for each difference score were estimated as $se = \sqrt{\frac{1}{N_{1} -3} +  \frac{1}{N_{2} -3} }$, with $N_1$ being the sample size in the original study and $N_2$ being the sample size in the replication study. This standard error is an approximation for the included F tests with a $df_1$ greater than 1 and chi square tests, and in order to check whether this was strongly impacting results all multilevel meta-analyses were re-performed excluding these studies. No differences in the substantive interpretation of results would follow from this change (i.e., $\gamma_0$ did not change by more than `r round(max(validSEDiff), 3)`, no statistical tests changed their statistical significance at the .05 alpha level, and variance partitioning did not change enough to alter interpretations).

#### Accounting for null results

##### Analysis 2 and 3: Multilevel random effects meta-analysis with exclusions

Analyses 2 and 3 reperform the above meta-analysis excluding studies using two exclusion criteria, (2) excluding studies in which the replication study was not statistically significant with an effect in the same direction as the original (using p value reported in the replication projects' datasets,  at an alpha of .05, and using two-tailed tests where applicable), and (3) by removing statistically equivalent studies. Because both of these methods function by removing small replication effects, no significance testing was performed on the difference between the model estimates estimated after with and without exclusions. 

The equivalence bound selected in analysis 3 was the lowest effect size that would have been statistically significant in the original study (assuming an alpha of .05) following a suggestion in {Lakens, 2018 #951}. Equivalence tests were performed using Z tests of the Fisher Z transformed effect sizes, excluding studies where the observed replication effect was significantly smaller than the equivalence bound using a one tailed test at the 95% confidence level. Standard errors of each study were estimated as $\sqrt{\frac{1}{n-3}}$, except for studies from {Camerer, 2018 #967} which had more than a single replication attempt, where standard errors are those derived from the meta-analyses that produced the effect size estimate (see supplementary materials [exclusion rules] for details).

##### Analysis 4: Bayesian mixture model

Finally, the Bayesian mixture model presented in {Camerer, 2018 #967} was also used to estimate the amount of effect size attenuation that should be expected between original and replication studies. This model assumes that each observed replication effect size comes from one of two components, either from the null-hypothesis or from the alternative-hypothesis component.  If the replication effect size is drawn from the null-hypothesis component, it is assumed to have come from a normal distribution with a mean 0 and a standard deviation equal to the standard error of the replication study (estimated here as $\sqrt{\frac{1}{n-3}}$, n being the replication sample size). If the replication effect size is sampled from the alternative distribution, it is assumed to have been drawn from a normal distribution with a standard deviation equal to the standard error of the replication study, and a mean equal to the true effect size. In this case, the true effect size is sampled from a normal distribution with a mean equal to the original study's estimated true effect size, attenuated by an "attenuation factor", equal to some value between zero and one. The attenuation factor is constrained to be equal across all studies. There are two main parameters of interest in this model; the "attenuation factor" (called a deflation factor in {Camerer, 2018 #967}), the degree to which effect sizes are attenuated between original and replication studies, and the overall rate at which studies are assigned to have come from the null hypothesis (the "assignment rate"). This analysis was performed in JAGS version 4.3.0 {Depaoli, 2016 #1010} using the rjags interface (version 4.8.0; {Plummer, 2018 #1011}). See supplementary materials ["mixture model"] for model syntax and further analysis details.

## Supplementary material 
## SM1 
###  Replication project Extraction and exclusion details

##### {Open Science Collaboration, 2015 #611}
Three original studies which did not report that their findings were indicative of a non-zero effect were excluded from those studies extracted from {Open Science Collaboration, 2015 #611}. Three studies for which z transformed correlation coefficients could not be extracted due to missing data in the downloaded data set were also excluded from analysis. Effect sizes for original and replication studies are included for `r nrow(data)` out of 97 studies replicated studies from {Open Science Collaboration, 2015 #611} which reported having found a non-zero effect. 

##### {Camerer, 2018 #967}
Original and replication effect sizes were extracted for all `r nrow(data4)` studies included in {Camerer, 2018 #967}. In some cases in the Nature Science reproducibility project {Camerer, 2018 #967} multiple replication studies were performed for a single effect. In each of these cases we performed a fixed effects meta-analysis using the metafor package {Viechtbauer, 2010 #796} to estimate a meta-analytic effect size estimate. The effect size, standard errors and sample sizes used in the current study reflect this pooled estimate. This method leads to one study more "replicating" according to the 'statistical significance in the same direction of the original study' criterion than was originally reported in {Camerer, 2018 #967}, where they used the p value from the largest performed study instead of a pooled estimate. 

##### {Soto, in press #1032} 
Effect sizes were extracted for original and replication studies for `r sum(!is.na(data7$correlation.o) & !is.na(data7$correlation.r))` out of `r nrow(loopr)` included studies, and one original study's sample size was not available. In {Soto, in press #1032} effect sizes which were only reported in this dataset as beta coefficients were not converted to Fisher z scores as not enough information was available in the data set to do so. A total of `r sum(allData$abrev == "loopr")` of 121 effects were included in the current analysis. As some replication studies used shorter form versions of the original data collection instruments, all results presented have been disattenuated using the Spearman-Brown prediction formula and Spearman disattenuation formula to estimate the trait-outcome associations that would be expected if the outcome measure had used the same number of items as the original study (Lord & Novick, 1968). Following the other large-scale replication studies, the signs of the original and replication study effects were inverted for analysis if the original effect was negative. 

##### {Cova, 2018 #984}
{Cova, 2018 #984} included three replications of original studies which were non-significant (and which did not claim to provide evidence for the effects under test), these were removed from analysis. Effect sizes were reported by Cova et al. (2018) and are included in the current study for `r sum(!is.na(data6$fis.o) & !is.na(data6$fis.r) & !is.na(data6$n.o) & !is.na(data6$n.r))`, original and replication studies, out of an original 37 replicated studies with significant original results. The four studies for which no effect sizes were reported performed analyses for which Cova et al. (2018) could not develop reasonable effect size estimates (e.g., a Sobel test, GEE analysis).

##### Many labs 1 {Klein, 2014 #988}
Many labs 1 {Klein, 2014 #988} examined whether effects from 13 original papers replicated, one of which did not report an effect size or  test statistic so is not included in the current sample. No effect size was extractable for one original study, and this effect was excluded for the purposes of the current analysis. Four different operationalisations of anchoring effects were tested, all of which are included in the current analysis, leading to a total of 15 paired data-points being included from this study. The multilevel models reported below accounts for non-independence between effects by including a random effect for study. 

##### Many labs 2 {Klein, 2018 #1021}
A total of `r nrow(data8)` of `r nrow(data8) + 6 ` paired original and replication effects sizes are included for this analysis. Four studies from {Klein, 2018 #1021}  were removed because the original and replication studies examined a difference in effect sizes seen in different conditions, and the effects were not directly tested against each other making it difficult to derive an appropriate effect size. Two additional studies were excluded because their effect sizes were only available as Cohen's q.

##### {Ebersole, 2016 #985}
Original and replication effect sizes were extracted for all 9 original and replication studies from {Ebersole, 2016 #985}, excluding a study they term a "conceptual replication". Most effects (6/9) were converted to correlation coefficients from the Cohen's d values reported in this replication project. The results of three additional studies reported as partial Eta squared were converted to correlation coefficients from F statistics using the formula: 

$r\  =\ \sqrt{\frac{F_{obs}\times(df_1 / df_2)}{F_{obs} \times (df_1 / df_2) + 1}} \times \sqrt{\frac{1}{df_1}}$

##### {Camerer, 2016 #983}
The economics replication project {Camerer, 2016 #983}. Original and replication effect sizes for all `r sum(allData$abrev == 'econ')` studies were reported in correlation coefficients and all are included in this analysis.

### SM2 
### Plots and multilevel model output of the relationship between original and replication correlation coefficients using varied exclusion criteria

The following output shows scatter plots and model output for all of the multilevel meta-analyses performed using the varied exclusion criteria explained in the main text.

Table SM `r tabSMN <- 1; tabSMN`. Multilevel meta-analysis model estimates and random effects for all data.

```{r}
kable(niceModelSums$`All Data`)
```

```{r}
plotAllData 
```

Figure SM`r figSMN <- 1; figSMN`. Scatter plot of replication effect sizes (in correlation coefficients) plotted against original effects including all data.


Table SM`r tabSMN <- tabSMN + 1; tabSMN`. Multilevel meta-analysis model estimates and random effects including only statistically significant replications. 

```{r}
kable(niceModelSums$`Only Signficant replications`)
```


```{r}
plotSigR 
```

Figure SM`r figSMN <- figSMN+1; figSMN`. Scatter plot of replication effect sizes (in correlation coefficients) plotted against original effects including only statistically significant replications. 


Table SM`r tabSMN <- tabSMN + 1; tabSMN`. Multilevel meta-analysis model estimates and random effects including studies which are not statistically equivalent to the null, using equivalence bounds set as the minimum effect size that would have been statistically significant in the original study. 

```{r}
kable(niceModelSums$`Non-equivalent studies`)
```

```{r}
plotNonequiv 
```

Figure SM`r figSMN <- figSMN+1; figSMN`. Scatter plot of replication effect sizes (in correlation coefficients) plotted against original effects including studies which are not statistically equivalent to the null, using equivalence bounds set as the minimum effect size that would have been statistically significant in the original study.


### SM3
#### LOO Cross validation output

#### Table [LOO cross validation output].
0th, 25th, 50th, 75th and 100th percentiles from leave one out cross validation for each multilevel model, for each exclusion method an, including only the sample indicated in "LOO exclusions". 

```{r}
kable(LOOoutput)
```

### SM4
#### Bayesian Mixture Model

The mixture model results presented in text presents the model developed by Camerer et al,. (2018; see https://osf.io/xhj4d/ for their detailed description of this model).  All priors were chosen to be uninformative or vague. The mixture model assumes that the observed replication effect sizes either come from the null hypothesis, a true effect sampled from a normal distribution with a mean of zero and a estimated precision (tau). This model uses an errors-in-variables approach to account for possible attenuation of effect sizes due to measurement error and estimation uncertainty following {Matzke, 2017 #1012}, which means the effect size attenuation factor is the factor change between the estimated true effect of the original and replication study effect size. Although this may be reasonable in that the true effect size of the effect may not be the true effect size of a particular study and analysis set up, this poses an interpretative problem in that alpha now represents the difference between the estimated original effect and the replication effect.

Box SM1. The original model reported in {Camerer, 2018 #967} and reported on in the main text of the current article. 

```{}
model{
# Mixture Model Priors:
alpha ~ dunif(0,1) # flat prior on slope for predicted effect size under H1
tau ~ dgamma(0.001,0.001) # vague prior on study precision
phi ~ dbeta(1, 1) # flat prior on the true effect rate
# prior on true effect size of original studies:
for (i in 1:n){
trueOrgEffect[i] ~ dnorm(0, 1)
}
# Mixture Model Likelihood:
for(i in 1:n){
clust[i] ~ dbern(phi)
# extract errors in variables (FT stands for Fisher-transformed):
orgEffect_FT[i] ~ dnorm(trueOrgEffect[i], orgTau[i])
repEffect_FT[i] ~ dnorm(trueRepEffect[i], repTau[i])
trueRepEffect[i] ~ dnorm(mu[i], tau)
# if clust[i] = 0 then H0 is true; if clust[i] = 1 then H1 is true and
# the replication effect is a function of the original effect:
mu[i] <- alpha * trueOrgEffect[i] * equals(clust[i], 1)
# when clust[i] = 0, then mu[i] = 0;
# when clust[i] = 1, then mu[i] = alpha * trueOrgEffect[i]
  }
}
```

### SM5

#### Conversions

All statistical tests extracted were transformed into correlation coefficients as follows, using the methods reported in {Open Science Collaboration, 2015 #611}. 

t statistics:

$r\ =\ \sqrt{\frac{t_{obs}^2\times(1 / df)}{(t_{obs}^2 / df) + 1}}$

Where $t_{obs}$ is the observed t statistic and $df$ is the degrees of freedom of the t test. 

F statistics:

$r\  =\ \sqrt{\frac{F_{obs}\times(df_1 / df_2)}{F_{obs} \times (df_1 / df_2) + 1}} \times \sqrt{\frac{1}{df_1}}$

Where $F_{obs}$ is the observed F statistic and $df_1$ is the degrees of freedom of the numerator and $df_2$ is degrees of freedom of the denominator.  

Chi square statistics:

$r\ =\ \sqrt{\frac{\chi_{obs}}{df + 2}}$

Where $chi_{obs}$ is the observed $chi^2$ statistic and $df$ is the associated degrees of freedom.

All values were then transformed into fisher Z transformed correlation coefficients using:

$z\ = \frac 12 \times \ln{\Big(\frac{1 + r}{1 - r}}\Big)$
  
  
