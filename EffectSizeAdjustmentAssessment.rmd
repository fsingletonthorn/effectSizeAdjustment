---
title: "R Notebook"
output: html_notebook
---


```{r message=FALSE, warning=FALSE, echo=FALSE}
source(file = 'Data/data_collection_cleaning.R')

```


## Introduction
See word document 


## Methods 
### Data extraction
All of the large scale replication projects that have been performed in behavioural science reserach were collected. The original source of each study, test statistics, effect sizes, sample sizes, standard errors, p-values were extracted for each original and replication study. Several of the large scale replication projects did not present the original test statistics and p values (e.g., Many labs 1 and 3). In these cases, these values were manually extracted from the original articles. When sample sizes for original studies were not avaliable they were manually extracted from original articles. When the original and replication effect sizes were not reported as Fisher Z transformed correlation coefficents, effect sizes were converted from test statistics or effect sizes for analysis. In cases where sample sizes were not reported per group, equal sample sizes among groups were assumed to be equal in these estimates. See table one for the number of valid studies extracted from each project. All results are reported in correlation coefficients following {Open Science Collaboration, 2015 #611} in order to present results in a common metric which is likely intuitivly understandable and familiar to most psychologists and behavioural reserachers. 

Three studies which did not report that their findings were indicitive of a true effect were excluded from {Open Science Collaboration, 2015 #611}. For the Nature Science reporducibility projects {Camerer, 2018 #967}, when multiple replciation studies were run, a fixed effects meta-analysis was performed using the metafor package {Viechtbauer, 2010 #796} for each study to estimate the true effect. P values, standard errors and ns reflect this pooled estiamte. This method leads to one study more "replicating" according to the 'statistical significance in the same direction of the original study' than was originally reported in the nature science project, where they using the largest perfored study instead of a pooled estiamte.

In the LOOPR study CITATION, some measures used shorter form version of the original questionaire, all results presented have been disattenuated using the Spearman-Brown prediction formula and Spearman disattenuation formula to estimate the trait-outcome associations that would be expected if our outcome measure had used the same number of items as the original study (Lord & Novick, 1968). Following the other large scale replication studies, the signs of negative original correlations were set to positive (and the sign of the replicaiton sample were switched too). 

The experimental philosophy reproducibility project included 2 origianl studies which were non-significant (and which were not claimed to provide evidence for the effects under test), these were removed from analysis. 

Many labs 2 [CITATION] original p values were recalculated from summary statitsics (i.e., from Cohen's d). Four studies from this reporducibility project were removed because effect sizes could not be simply derrived (the original and replication studies examined a differnece in effect sizes seen in different conditions, and the effects were not directly tested against each other), and two additional were excluded because their effect sizes were only avalible in Cohen's q.

INSERT TABLE 1 HERE

### Analysis
All analysis was perfomred in R {R Development Core Team, 2018 #314} using the Metafor package {Viechtbauer, 2010 #796}. In order to obtain a reasonable estimate of the change in effect size between original and replication studies, a multilevel random effects meta-analysis was performed on the difference in Fisher Z transformed correlations between original and replication studies. Standard errors were estiamted as $\sqrt{\frac{1}{N_{1} -3} +  \frac{1}{N_{2} -3} }$, with $N_1$ being the sample size in the original study and $N_2$ being the sample size in the replication study. Empirical Bayes estimates of the random effects were obtained following {Robinson, 1991 #999}. 

Confidence intervals around binomial proportions are 95% Wilson Score intervals. Replication Bayes factors were calculated following 

### What would be a reasonable blanket rule for an effect that no longer matters?
I.e., an effect that  is .2 of the original effect? 

Bayesian test of r != 0 vs. r >/< 0, BF in which direction (assuming a flat or default prior)

All of the methods in the RPP study - original within bounds of 95% replication CI

EXCLUDE STUDIES - Find the smallest effect that would have been statistically significant in the original study and base the 95% CI on that. Lakens , Scheel and  Isager 2018 P 262




## Testing 

```{r message=FALSE}

ggplot(allData, aes(correlation.o, correlation.r,size = log(n.r), colour = as.factor(source))) +  geom_point(alpha = 1, na.rm = T)+  ochRe::scale_colour_ochre(palette = "tasmania") + theme_classic()  + ylim(c(-.5, 1))+ xlim(c(-.0, 1)) + scale_shape_manual(values = c(8,16)) 

# summary(abs(loopr$correlation.o))  
mean((allData$correlation.r-allData$correlation.o)/allData$correlation.o, na.rm = T)
mean((allData$fis.r -allData$fis.o)/allData$fis.o, na.rm = T)

# cor.test(allData$correlation.r, allData$correlation.o, na.rm = T)

# allData

```



### Testing methods of removing originals

```{r}
# extracting just the significant ones (i.e., successful replications according to p < .05 on the replication)





```

###







```{r}

```















TOST - 
Exclude inconclusive ones - 

Bayesian modelling - compare data for and against 








## https://osf.io/z7aux/
## HAVE TO GO THROUGH AND REMOVE THOSE BASED ON 




######## 
NOTE ! ! ! - it may be important to use disattenuated values from LOOPR because they used short form analyses









