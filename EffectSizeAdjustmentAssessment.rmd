---
title: "R Notebook"
output: html_notebook
---


```{r}
source(file = 'Data/data_collection_cleaning.R')
devtools::install_github('hollylkirk/ochRe')
library(ochRe)
```

### LIST OF ALL OF THE TYPES OF REPLICATION SUCCESS HERE ! ! ! ! !




# REMEMBER TO COUNT ALL OF THE ARTICLES INCLUDED FROM EACH SOURCE - all for which Rs could be calculated or extracted from original and replication studies

## Methods 
Many of the studies did not present the original test statistics and p values (Many labs 1, 3 (CHECK 2), ), which were manually extracted from the original articles.

When sample sizes for original studies were not avaliable they were manually extracted from original articles (for: many labs 3). Effect size statistics such as F values with a df1 of > 1, and χ2 values have been converted to correlations and Cohen's d values, but their standard errors cannot be computed directly. These studies have been excluded from the meta-analyses.

Sample sizes were assumed to be equal in all groups. Original p values and replication p values presented here were re-calculated using the Fisher Transformed z score.


For nature science reporducibility papers, when multiple replciation studies were run, a fixed effects meta-analysis was performed using the metafor package for each study to estimate the effect. P values, standard errors and ns reflect this pooled estiamte. This method lead to one study more "replicating" according to the 'statistical significance in the same direction of the original study' using the largest perfored study criteria that the original authors reported. 

# I COULD GO THROUGH AN RECALCULATE P VALUES FOR ALL THOSE WITH VALID SEs at the end



TRY SMALL TELESCOPES, lower bound of 95% CIs, other methods of adjustment I.e., (Anderson et al., 2017; McShane & Böckenholt, 2016; Perugini et al., 2014; and Taylor & Muller, 1996). Maybe look into some of the meta-analytic methods?


There are four main suggested methods for estimating effect sizes from single studies for use in power analysis, while accounting for publication bias. 

Small telescopes - 


Anderson et al, 2017 /  Taylor & Muller, 1996
Estimate the distribution assuming a truncated t distribtuion. Fun!  Probably do with and without removing the 0s for this one - as negative effects might be estimated at 0 / negative.  


McShane & Böckenholt, 2016
Power Calibrated Effect Size approach ~~ Perform and check which effect size it is equivilent to selecting as an estimate of the effect size
 SEE: McShane, B. B., & Böckenholt, U. (2016). Planning sample sizes when effect sizes are uncertain The power-calibrated effect size approach.R for methods for this. 
 
 Note this is meant to provide a specific thing, i.e., an estiamte accounting for uncertainty not a an estimate of the true effect sans publcaiton bias, and properly should be used on the second study too. 


Perugini et al., 2014
Safeguard power - the lower bound of the xx% CI. 


### What would be a reasonable blanket rule for an effect that no longer matters?
I.e., an effect that  is .2 of the original effect? 

Bayesian test of r != 0 vs. r >/< 0, BF in which direction (assuming a flat or default prior)

All of the methods in the RPP study - original within bounds of 95% replication CI

EXCLUDE STUDIES - Find the smallest effect that would have been statistically significant in the original study and base the 95% CI on that. Lakens , Scheel and  Isager 2018 P 262




#### Conversion
All effect sizes were converted into correlation coefficents, or extracted as such. Only effect sizes which could be converted into correlation coefficents are used in the current analysis. 


This was done for two primary reasons. Most of the large scale replication projects have primarily reported their results in Pearson's r (i.e., all but one of the projects included in the current research). Secondarily, r values are broadly well understood and commonly used by psychologists. 

One of the benifits of this approach is that standard errors can be developed for the Cohen's z transform of the correlation coefficents, and for differneces between z-transformed correlation coefficients, if values were developed from test statstics of r, t, or F(1,df2) following (Open Science Collaboration, 2015). However, standard errors developed this way are not valid for F statistics with a $df_1$ of > 1 or chi square statistics. These studies have been exlcuded from analyses which require these values [...!...]. 


Effects which were non-significant were included following the inclusion rules used by studies includied - if the original authors reported them as significant. This means that some of the p values for included studies were in fact over .05. Results which were not reported as significant were not included. 

Because in the loopr study, some measures used shorter form version of the original results, all results presented have been disattenuated using the Spearman-Brown prediction formula and Spearman disattenuation formula to estimate the trait-outcome associations that would be expected if our outcome measure had used the same number of items as the original study (Lord & Novick, 1968). Following the other large scale replication studies, the signs of negative original correlations were set to positive (and the sign of the replicaiton sample were switched too). 



IT MAY BE WORTH EXLUDING THOSE NON-sig few from the begining

```{r message=FALSE}



ggplot(tmp, aes(correlation.o, correlation.r,size = log(n.r), colour = as.factor(source))) +  geom_point(na.rm = T)+  ochRe::scale_colour_ochre(palette = "tasmania") + theme_classic()  + ylim(c(-.5, 1))+ xlim(c(-.5, 1)) # scale_shape_manual(values = c(0,1,2,15,16,17)) +
# summary(abs(loopr$correlation.o))  


ggplot(tmp[as.numeric(tmp$pVal.r) <.05 | is.na(as.numeric(tmp$pVal.r)),], aes(correlation.o, correlation.r,size = log(n.r), colour = as.factor(source))) +  geom_point(na.rm = T)+  ochRe::scale_colour_ochre(palette = "tasmania") + theme_classic() + ylim(c(-.5, 1))+ xlim(c(-.5, 1)) # scale_shape_manual(values = c(0,1,2,15,16,17)) +

mean((tmp$correlation.r-tmp$correlation.o)/tmp$correlation.o, na.rm = T)
mean((tmp$correlation.r-tmp$correlation.o), na.rm = T)

cor.test(tmp$correlation.r, tmp$correlation.o, na.rm = T)


```



### Testing methods of removing originals

```{r}


# extracting just the significant ones (i.e., successful replications according to p < .05 on the replication)
dat <- tmp[as.numeric(tmp$pVal.r) <.05 | is.na(as.numeric(tmp$pVal.r)),]

mean((dat$correlation.r-dat$correlation.o)/dat$correlation.o, na.rm = T)
mean((dat$correlation.r-dat$correlation.o), na.rm = T)
mean(data.frame(dat$correlation.r[-30], dat$correlation.o[-30], (dat$correlation.r[-30]-dat$correlation.o[-30])/dat$correlation.o[-30])[,3], na.rm = T)
cor.test(dat$correlation.r, dat$correlation.o, na.rm = T) 
```

###



TOST - 
Exclude inconclusive ones - 

Bayesian modelling - compare data for and against 








## https://osf.io/z7aux/
## HAVE TO GO THROUGH AND REMOVE THOSE BASED ON 




######## 
NOTE ! ! ! - it may be important to use disattenuated values from LOOPR because they used short form analyses









